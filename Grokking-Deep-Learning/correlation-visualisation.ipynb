{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local correlation summarisation\n",
    "When a given set of weights is configured such that it is able to correlate its input layer to what the output <em>should be</em>.\n",
    "\n",
    "### Global correlation summarisation\n",
    "What an earlier layer 'says it should be' can be determined by taking what a later layer 'says it\n",
    "should be' and multiplying it by the weights in between them. This way, <em>later layers can tell\n",
    "earlier layers</em> what kind of signal they need to ultimately find correlation with the output. This is the purpose of backpropagation.\n",
    "\n",
    "Analogy: when a neuron in the final layer says, “I need to be a little higher,” it then\n",
    "proceeds to tell all the neurons in the layer immediately preceding it, “Hey, previous layer,\n",
    "send me higher signal.” They then tell the neurons preceding them, “Hey. Send us higher\n",
    "signal.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A better visualisation of the previous 3-layer neural network:\n",
    "Rather than a graphical representation, we can visualise neural networks as just a chain of vectors and matrices: \n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "  <img src=\"img/chap7_visualisation_1.png\" width=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dimensions of the weight matrix between any two layers can be inferred from the dimensions of the two layers, the visualisation can be simplified further to:\n",
    "<p>\n",
    "  <img src=\"img/chap7_visualisation_2.png\" width=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "We can fully represent the whole forward propagation with just this equation: $l_2=\\text{relu}(l_0 W_0)W_1$, where $l_i$ is the $i^{th}$ layer and $W_i$ is the weight matrix between layers $l_i$ and $l_{i+1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network Architecture:\n",
    "A neural network's <em>architecture</em> is its particular configuration of layers and weights. <em>Different architectures can make certain correlations easier to discover.</em> Additionally, a good architecture will filter out the signal in the noise and prevent overfitting. A significant part of neural network research is about discovering new architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
