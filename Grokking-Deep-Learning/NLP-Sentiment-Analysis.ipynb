{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing:\n",
    "It is necessary to translate the input text into matrices that can be fed into a neural network and receive a numerical prediction.\n",
    "\n",
    "The way we format the input text as a matrix will make it easier or harder for the neural network to learn correlation.\n",
    "\n",
    "### Sentiment Analysis:\n",
    "<em>One-hot encoding</em> involves using binary numbers 0 and 1 to encode the presence/absence of something or truth/false. For sentiment analysis, an appropriate way to translate input text into vectors would be to have a column for each word in a vocabulary set, where if the current review contains a certain word, the column would be marked with a 1.\n",
    "\n",
    "### Word Embedding:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB Movie Review Sentiment Analysis:\n",
    "The following snippet transforms the raw review data into vectors that can be passed as input into a neural network. There are two labels: positive sentiment and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 25000 samples.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "f = open('./Datasets/imdb_reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('./Datasets/imdb_review_labels.txt')\n",
    "raw_labels = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:set(x.split(\" \")),raw_reviews))\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        if(len(word)>0):\n",
    "            vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "\n",
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word]=i\n",
    "\n",
    "input_dataset = list()\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(list(set(sent_indices)))\n",
    "\n",
    "target_dataset = list()\n",
    "for label in raw_labels:\n",
    "    if label == 'positive\\n':\n",
    "        target_dataset.append(1)\n",
    "    else:\n",
    "        target_dataset.append(0)\n",
    "\n",
    "print(\"Prepared {} samples.\".format(len(input_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An example datapoint:\n",
      "===== Raw =====\n",
      "bromwell high is a cartoon comedy . it ran at the same time as some other programs about school life  such as  teachers  . my   years in the teaching profession lead me to believe that bromwell high  s satire is much closer to reality than is  teachers  . the scramble to survive financially  the insightful students who can see right through their pathetic teachers  pomp  the pettiness of the whole situation  all remind me of the schools i knew and their students . when i saw the episode in which a student repeatedly tried to burn down the school  i immediately recalled . . . . . . . . . at . . . . . . . . . . high . a classic line inspector i  m here to sack one of your teachers . student welcome to bromwell high . i expect that many adults of my age think that bromwell high is far fetched . what a pity that it isn  t   \n",
      " ===> Label: positive\n",
      "\n",
      "===== Processed =====\n",
      "20496 34834 71701 20502 49181 29216 65066 37931 57390 4656 20016 12852 39477 33358 7764 65114 35425 49250 56939 38509 57457 26742 30337 72323 41093 54918 3207 54919 29853 46755 6834 40645 39110 11463 14029 36047 59602 36050 21714 21207 59103 16607 42723 6898 25334 16647 31501 70418 26902 15128 9506 17198 17714 12090 39227 26941 46909 70976 56131 44357 51023 3934 63330 19301 14182 68463 46451 66421 68473 25985 56203 34703 41364 41377 47528 22447 25014 66999 952 53694 64958 37312 58305 13763 23500 60876 49614 24030 3554 49648 66036 24568 35323 \n",
      "===> Label: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"An example datapoint:\")\n",
    "print(\"===== Raw =====\")\n",
    "for i in raw_reviews[0].split(\" \"):\n",
    "    print(i, end=\" \")\n",
    "print(\"===> Label: {}\".format(raw_labels[0]))\n",
    "print(\"===== Processed =====\")\n",
    "for i in input_dataset[0]:\n",
    "    print(i, end=\" \")\n",
    "print(\"\\n===> Label: {}\".format(target_dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 3-layer network below takes in a 'sentence', represented as a variable length array consisting of integers which encode the index of a particular word in the vocabulary exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter:0 Progress:95.99% Training Accuracy:0.83170833333333346\n",
      "Iter:1 Progress:95.99% Training Accuracy:0.8664583333333333\n",
      "Iter:2 Progress:95.99% Training Accuracy:0.8847361111111111\n",
      "Test accuracy: 0.854\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "alpha, iterations = (0.01, 3)\n",
    "hidden_size = 100\n",
    "\n",
    "weights_0_1 = 0.2 * np.random.random((len(vocab), hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, 1)) - 0.1\n",
    "\n",
    "correct, total = (0, 0)\n",
    "for iteration in range(iterations):\n",
    "    for i in range(len(input_dataset) - 1000):    # Leave 1000 datapoints for the testing\n",
    "        x, y = (input_dataset[i], target_dataset[i])\n",
    "        # Embed and apply sigmoid:\n",
    "        layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "        layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "        layer_2_delta = layer_2 - y\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T)\n",
    "        weights_0_1[x] -= layer_1_delta * alpha\n",
    "        weights_1_2 -= np.outer(layer_1,layer_2_delta) * alpha\n",
    "        if (np.abs(layer_2_delta) < 0.5):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        if (i % 10 == 9):\n",
    "            progress = str(i / float(len(input_dataset)))\n",
    "            sys.stdout.write('\\rIter:' + str(iteration) \\\n",
    "                             + ' Progress:' + progress[2:4] \\\n",
    "                             + '.' + progress[4:6] \\\n",
    "                             + '% Training Accuracy:' \\\n",
    "                             + str(correct / float(total)))\n",
    "    print()\n",
    "correct, total = (0, 0)\n",
    "for i in range(len(input_dataset) - 1000, len(input_dataset)):\n",
    "    x = input_dataset[i]\n",
    "    y = target_dataset[i]\n",
    "    layer_1 = sigmoid(np.sum(weights_0_1[x], axis=0))\n",
    "    layer_2 = sigmoid(np.dot(layer_1, weights_1_2))\n",
    "    \n",
    "    if (np.abs(layer_2 - y) < 0.5):\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print(\"Test accuracy: {}\".format(correct / float(total)))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note about neural architecture:\n",
    "The most basic ability of a neural network is to learn <em>direct correlation</em>. The minimal architecture for identifying direct correlation is a 2-layer network. \n",
    "\n",
    "Hidden layers search for 'useful' groupings of inputs from the previous layer. Each hidden layer neuron takes in input from the previous layer and asks 'is this datapoint in my group?' These groupings must be useful to the prediction of an output label. \n",
    "\n",
    "Understanding the difference between 'terrible' and 'not terrible' is a powerful grouping for predicting the output labels 'negative' or 'positive'.\n",
    "\n",
    "The above neural network takes in vocabulary and neglects the effect of the ordering of words. Eg. the sentences 'the movie was terrible, not great' and 'the move was great, not terrible' cover the same set of vocabulary, but have opposite labels. If you can construct two inputs with differing patterns but which produce the same layer_1 (hidden layer) output, then the network is incapable of recognising that differing pattern.\n",
    "\n",
    "Words with similar predictive power should have similar weights leading to a similar set of hidden neurons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarity:\n",
    "The snippet below computes the similarity of words using <em>Euclidean distance</em>. The vector of weights of one word (input neuron) leading to each hidden neuron of the next layer should be 'close' in terms of Euclidean distance to the vector of weights for another word if those two words have similar predictive power (since they would be subscribing with similar strength to the same nodes of the hidden layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words similar to 'Beautiful'\n",
      "[('beautiful', -0.0),\n",
      " ('thank', -0.6965802923245741),\n",
      " ('spirit', -0.7055560560453192),\n",
      " ('spot', -0.7373386929316352),\n",
      " ('success', -0.7412047577955438),\n",
      " ('heart', -0.7418008650600879),\n",
      " ('charlie', -0.7449509776260076),\n",
      " ('eerie', -0.7453403924544343),\n",
      " ('extraordinary', -0.7495099205406576),\n",
      " ('great', -0.7527903303113145)]\n",
      "Words similar to 'Terrible'\n",
      "[('terrible', -0.0),\n",
      " ('worse', -0.7230350835769849),\n",
      " ('badly', -0.7502508378281694),\n",
      " ('fails', -0.7623534149853021),\n",
      " ('horrible', -0.7995433462398092),\n",
      " ('laughable', -0.8179713041032471),\n",
      " ('dull', -0.8367748288666471),\n",
      " ('redeeming', -0.8381812642109021),\n",
      " ('basically', -0.8393347151649787),\n",
      " ('lame', -0.8451263953208726)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import math\n",
    "\n",
    "def similar(target):\n",
    "    target_index = word2index[target]\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "print(\"Words similar to 'Beautiful'\")\n",
    "pprint(similar(\"beautiful\"), width=100)\n",
    "print(\"Words similar to 'Terrible'\")\n",
    "pprint(similar(\"terrible\"), width=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill In The Blank:\n",
    "Instead of taking in a review and predicting 'positive' or 'negative', this problem involves taking a sentence with a missing word and selecting the most appropriate word to fill the sentence. This problem requires the context of the sentence to be taken into account, which wasn't reflected well in the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Vocab' size: 74075\n",
      "Sample vocab: ['rebeecca', '', 'weeps', 'helmer', 'dishonor']\n",
      "'Concatenated' size: 7459318\n",
      "Sample concatenated: [71357 49779  6590 62870 65829]\n",
      "input_dataset size: 25000\n",
      "Sample input_dataset: [1, 1, 1099, 45369, 1, 6590, 62870, 24964, 71996, 5565] ... and more vocab indices\n"
     ]
    }
   ],
   "source": [
    "import sys, random, math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "f = open(\"./Datasets/imdb_reviews.txt\")\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "# Get a collection of words\n",
    "tokens = list(map(lambda x: (x.split(\" \")), raw_reviews))\n",
    "wordcnt = Counter()\n",
    "for sentence in tokens:\n",
    "    for word in sentence:\n",
    "        wordcnt[word] -= 1\n",
    "        \n",
    "# Form the vocabulary and word to index mapping dictionary\n",
    "vocab = list(set(map(lambda x: x[0], wordcnt.most_common())))\n",
    "word2index = {}\n",
    "for i, word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "# \n",
    "concatenated = []\n",
    "input_dataset = []\n",
    "for sentence in tokens:\n",
    "    sent_indices = []\n",
    "    for word in sentence:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)\n",
    "random.shuffle(input_dataset)\n",
    "\n",
    "print(\"'Vocab' size: {}\".format(len(vocab)))\n",
    "print(\"Sample vocab: {}\".format(vocab[:5]))\n",
    "print(\"'Concatenated' size: {}\".format(len(concatenated)))\n",
    "print(\"Sample concatenated: {}\".format(concatenated[:5]))\n",
    "print(\"input_dataset size: {}\".format(len(input_dataset)))\n",
    "print(\"Sample input_dataset: {} ... and more vocab indices\".format(input_dataset[0][:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.14998  [('terrible', -0.0), ('horrible', -2.291300288404048), ('thin', -2.4169675788743836), ('fantastic', -2.5089735852761117), ('brilliant', -2.5436852956924065), ('lame', -2.5514146967813307), ('dumb', -2.6072069177285813), ('fascinating', -2.613801067798473), ('weak', -2.664379906619758), ('terrific', -2.6724644833831124)])]8247)]090493)]"
     ]
    }
   ],
   "source": [
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (50, 2, 5)\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab), hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab), hidden_size) * 0\n",
    "\n",
    "layer_2_target = np.zeros(negative + 1)\n",
    "layer_2_target[0] = 1\n",
    "\n",
    "def similar(target):\n",
    "    target_index = word2index[target]\n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - (weights_0_1[target_index])\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 3-layer network:\n",
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "        # Predict labels from a smaller randomised subset, because it’s really expensive to predict from the entire vocabulary\n",
    "        # This seems crude, but it works well enough for the amount of speed we gain. \n",
    "        # TODO: This is called negative sampling, right?\n",
    "        target_samples = [review[target_i]] + list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        \n",
    "        left_context = review[max(0, target_i - window) : target_i]\n",
    "        right_context = review[target_i + 1 : min(len(review), target_i + window)]\n",
    "        \n",
    "        layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "        \n",
    "        weights_0_1[left_context + right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1) * alpha\n",
    "        \n",
    "    if rev_i % 250 == 0:\n",
    "        sys.stdout.write('\\rProgress: '+str(rev_i/float(len(input_dataset)*iterations)) + \"   \" + str(similar('terrible')))\n",
    "    sys.stdout.write('\\rProgress: ' + str(rev_i/float(len(input_dataset)*iterations)))\n",
    "print(similar('terrible'), width=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This networks clusters the words that are likely to occur in the same sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
