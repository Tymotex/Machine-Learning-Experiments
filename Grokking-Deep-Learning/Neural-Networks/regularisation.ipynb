{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "Regularisation is a set of methods for getting neural networks to <em>generalise</em> rather than <em>memorise</em>. The goal of regularisation is the combat overfitting in neural networks. A neural network that overfits is a neural network that has learned the <em>noise</em> in the dataset rather than the <em>true</em> signal.\n",
    "\n",
    "In maths, stats and computer science, regularisation generally means to 'add information in order to solve an ill-posed problem'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:349 Error:0.108 Correct:1.099"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "reluDerivative = lambda x: x >= 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        error += np.sum((labels[i : i + 1] - layer_2) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(labels[i : i + 1]))\n",
    "        layer_2_delta = labels[i : i + 1] - layer_2\n",
    "        layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * reluDerivative(layer_1)\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    sys.stdout.write(\"\\r\"+ \\\n",
    "                     \" I:\" + str(j)+ \\\n",
    "                     \" Error:\" + str(error / float(len(images)))[0:5] +\\\n",
    "                     \" Correct:\" + str(correct_count / float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is done, we can test that it generalised well enough to handle the unseen dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test-Err:0.653 Test-Acc:0.7073\n"
     ]
    }
   ],
   "source": [
    "if (j % 10 == 0 or j == iterations - 1):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        error += np.sum((layer_2 - test_labels[i : i + 1]) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[i : i + 1]))\n",
    "    sys.stdout.write(\" Test-Err:\" + str(error / float(len(test_images)))[0:5] + \\\n",
    "                     \" Test-Acc:\" + str(correct_count / float(len(test_images))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Test accuracy</em> &mdash; the accuracy of the predictions made by the neural network on the training dataset.\n",
    "\n",
    "### Example of Overfitting:\n",
    "The following snippet trains the neural network on varying number of iterations and logs both the train accuracy and the test accuracy. Note that this takes a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration: 1 | Train Error: 0.722 | Train Acc:0.537 | Test Error: 0.601 | Test Acc:0.6488\n",
      " Iteration: 10 | Train Error: 0.312 | Train Acc:0.901 | Test Error: 0.420 | Test Acc:0.8114\n",
      " Iteration: 20 | Train Error: 0.232 | Train Acc:0.946 | Test Error: 0.417 | Test Acc:0.8066\n",
      " Iteration: 30 | Train Error: 0.194 | Train Acc:0.967 | Test Error: 0.448 | Test Acc:0.7921\n",
      " Iteration: 40 | Train Error: 0.166 | Train Acc:0.984 | Test Error: 0.482 | Test Acc:0.7706\n",
      " Iteration: 50 | Train Error: 0.145 | Train Acc:0.991 | Test Error: 0.513 | Test Acc:0.7558\n",
      " Iteration: 60 | Train Error: 0.127 | Train Acc:0.998 | Test Error: 0.544 | Test Acc:0.7446\n",
      " Iteration: 70 | Train Error: 0.115 | Train Acc:0.999 | Test Error: 0.600 | Test Acc:0.723\n",
      " Iteration: 80 | Train Error: 0.108 | Train Acc:0.999 | Test Error: 0.662 | Test Acc:0.704\n",
      " Iteration: 90 | Train Error: 0.105 | Train Acc:0.998 | Test Error: 0.712 | Test Acc:0.6932\n",
      " Iteration: 100 | Train Error: 0.104 | Train Acc:0.999 | Test Error: 0.745 | Test Acc:0.6797\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "reluDerivative = lambda x: x >= 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for k in [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "    for j in range(k):\n",
    "        error, correct_count = (0.0, 0)\n",
    "        for i in range(len(images)):\n",
    "            layer_0 = images[i : i + 1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            error += np.sum((labels[i : i + 1] - layer_2) ** 2)\n",
    "            correct_count += int(np.argmax(layer_2) == np.argmax(labels[i : i + 1]))\n",
    "            layer_2_delta = labels[i : i + 1] - layer_2\n",
    "            layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * reluDerivative(layer_1)\n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    test_error, test_correct_count = (0.0, 0)\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        test_error += np.sum((layer_2 - test_labels[i : i + 1]) ** 2)\n",
    "        test_correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[i : i + 1]))\n",
    "    \n",
    "    print(\"\\r\"+ \\\n",
    "          \" Iteration: {}\".format(j + 1) + \\\n",
    "          \" | Train Error: \" + str(error / float(len(images)))[0:5] + \\\n",
    "          \" | Train Acc: \" + str(correct_count / float(len(images))) + \\\n",
    "          \" | Test Error: \" + str(test_error / float(len(test_images)))[0:5] + \\\n",
    "          \" | Test Acc: \" + str(test_correct_count / float(len(test_images))))\n",
    "          \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the test accuracy peaks within the first 20 or so iterations, and then declines as the number of iterations grows. This happens while training accuracy converges to 100% and the training error converges to 0.\n",
    "\n",
    "How would we get neural networks to train only on the true signal rather than overfit to the noise in the training dataset? A simple strategy is <em>early stopping</em> &mdash; just not letting the network train long enough to overfit the training dataset. To determine when to stop, we run the model against <em>validation datasets</em> and stop at the peak accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Regularisation:\n",
    "A regularisation technique that involves randomly setting node values to zero (as well as their associated delta). This is simple but happens to be a state-of-the-art technique for combating overfitting. From a very high level, it works by training smaller subsections of the neural network &mdash; which don't overfit as easily (simply because smaller neural networks have less expressive power and can only capture prominent features). The randomness will mean that many different subsections will be targeted. A bigger neural network tends to overfit because it's able to capture, to a greater degree, the finer details of each sample in the training data which prevents it from generalising (the finer details often part of the noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting note: any two intially randomised neural networks will learn differently. It's quite unlikely that because of the initial random configuration of their weights that any two neural networks will learn to overfit to <em>same</em> noise.\n",
    "Now, suppose we have 100 neural networks each with randomised weights. It happens that when neural networks train, they will capture the broader signal first, and then start placing greater weights on the noise. With 100 different neural networks capturing different noisy aspects, taking the average of them will tend to cancel out their mistakes and leave the common signal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I:0 Test-Err:0.638 Test-Acc:0.6325 Train-Err:0.902 Train-Acc:0.395\n",
      "I:10 Test-Err:0.371 Test-Acc:0.8314 Train-Err:0.392 Train-Acc:0.819\n",
      "I:20 Test-Err:0.316 Test-Acc:0.8551 Train-Err:0.320 Train-Acc:0.884\n",
      "I:30 Test-Err:0.307 Test-Acc:0.8661 Train-Err:0.276 Train-Acc:0.918\n",
      "I:40 Test-Err:0.293 Test-Acc:0.8721 Train-Err:0.259 Train-Acc:0.931\n",
      "I:50 Test-Err:0.281 Test-Acc:0.8749 Train-Err:0.234 Train-Acc:0.94\n",
      "I:60 Test-Err:0.290 Test-Acc:0.8791 Train-Err:0.229 Train-Acc:0.957\n",
      "I:70 Test-Err:0.295 Test-Acc:0.8758 Train-Err:0.226 Train-Acc:0.958\n",
      "I:80 Test-Err:0.290 Test-Acc:0.8782 Train-Err:0.210 Train-Acc:0.961\n",
      "I:90 Test-Err:0.285 Test-Acc:0.8809 Train-Err:0.204 Train-Acc:0.95\n",
      "I:100 Test-Err:0.277 Test-Acc:0.8808 Train-Err:0.192 Train-Acc:0.97\n",
      "I:110 Test-Err:0.284 Test-Acc:0.8786 Train-Err:0.183 Train-Acc:0.975\n",
      "I:120 Test-Err:0.277 Test-Acc:0.8809 Train-Err:0.188 Train-Acc:0.97\n",
      "I:130 Test-Err:0.281 Test-Acc:0.8784 Train-Err:0.187 Train-Acc:0.963\n",
      "I:140 Test-Err:0.289 Test-Acc:0.8777 Train-Err:0.195 Train-Acc:0.969\n",
      "I:150 Test-Err:0.287 Test-Acc:0.8768 Train-Err:0.175 Train-Acc:0.973\n",
      "I:160 Test-Err:0.280 Test-Acc:0.8792 Train-Err:0.195 Train-Acc:0.968\n",
      "I:170 Test-Err:0.291 Test-Acc:0.876 Train-Err:0.174 Train-Acc:0.978"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "def relu(x):\n",
    "    return (x >= 0) * x\n",
    "\n",
    "def relu2deriv(x):\n",
    "    return (x > 0)\n",
    "\n",
    "alpha, iterations, hidden_size = (0.005, 300, 100)\n",
    "pixels_per_image, num_labels = (784, 10)\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image,hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size,num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_cnt = (0.0,0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i:i+1]\n",
    "        layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "        # NEW CODE: dropout_mask is a randomised matrix of 1s and 0s (50% chance for either 1 or 0 at each entry)\n",
    "        dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        # NEW CODE: switching off some nodes in layer_1 by applying the dropout_mask\n",
    "        # We're multiplying by 2 because we are turning off ~50% of the nodes in layer_1. If we don't, the layer_2\n",
    "        # prediction will be approximately halved. This means that layer_2_delta will be larger in magnitude. \n",
    "        # Multiplying by 2 will cheaply escape this effect.\n",
    "        layer_1 *= dropout_mask * 2\n",
    "        layer_2 = np.dot(layer_1,weights_1_2)\n",
    "        error += np.sum((labels[i:i+1] - layer_2) ** 2)\n",
    "        correct_cnt += int(np.argmax(layer_2) == np.argmax(labels[i:i+1]))\n",
    "        layer_2_delta = (labels[i:i+1] - layer_2)\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2.T) * relu2deriv(layer_1)\n",
    "        layer_1_delta *= dropout_mask\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    if(j % 10 == 0):\n",
    "        test_error = 0.0\n",
    "        test_correct_cnt = 0\n",
    "        for i in range(len(test_images)):\n",
    "            layer_0 = test_images[i:i+1]\n",
    "            layer_1 = relu(np.dot(layer_0,weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            test_error += np.sum((test_labels[i:i+1] - layer_2) ** 2)\n",
    "            test_correct_cnt += int(np.argmax(layer_2) == np.argmax(test_labels[i:i+1]))\n",
    "    \n",
    "        sys.stdout.write(\"\\n\" + \\\n",
    "            \"I:\" + str(j) + \\\n",
    "            \" Test-Err:\" + str(test_error/ float(len(test_images)))[0:5] +\\\n",
    "            \" Test-Acc:\" + str(test_correct_cnt/ float(len(test_images)))+\\\n",
    "            \" Train-Err:\" + str(error/ float(len(images)))[0:5] +\\\n",
    "            \" Train-Acc:\" + str(correct_cnt/ float(len(images))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Differences after implementing dropout:\n",
    "- Testing accuracy doesn't peak and drop as bad as it did without any regularisation\n",
    "- Traning accuracy takes longer to converge to 100%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
