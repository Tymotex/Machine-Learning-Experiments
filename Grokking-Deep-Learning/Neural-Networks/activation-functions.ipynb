{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions (Nonlinearities):\n",
    "An activation function is a function associated with a given neuron which takes in the weighted sum and operates on it, returning a number. The $\\texttt{relu}$ function is one such activation function. \n",
    "\n",
    "The activation function must/should have the properties:\n",
    "- Continuous at every $x$ in the real numbers\n",
    "- Injective &mdash; one unique output for each unique input. Simple tests: check the derivative is monotonically increasing or use the horizontal line test\n",
    "- Non-linear &mdash; literally means any function that isn't a pure straight line. The relu function is a piecewise function so it's not linear in that way. Having a non-linear function is necessary for <em>conditional correlation</em>\n",
    "- Should be efficient to compute, because the activation function could be called a massive number of times. There is usually a trade-off between the expressive power of the function and how fast it is to compute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Activation Functions &mdash; Hidden Layer:\n",
    "- $sigmoid$ &mdash; maps weighted sums to a value in $(0, 1)$. This lets you interpret the output of a neuron as a probability measure. $S(x)=\\frac{1}{1+e^{-x}}$\n",
    "\n",
    "<img src=\"img/sigmoid.png\" style=\"width: 35%\">\n",
    "\n",
    "- $tanh$ &mdash; maps weighted sums to a value in $(-1, 1)$. $tanh$ can give a measure of negative correlation (rather than just positive correlation in the case of sigmoid). Generally outperforms than sigmoid for hidden layers because its ability to measure negative correlation, although selecting which activation function to use depends on the application. $tanh(x)=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$\n",
    "\n",
    "<img src=\"img/tanh.png\" style=\"width: 35%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Activation Functions &mdash; Output Layer:\n",
    "The choice of activation function in the <em>output layer</em> depends on what prediction is being made.\n",
    "- $sigmoid$ &mdash; for yes/no probability predictions. Eg. is this a dog?\n",
    "- $softmax$ &mdash; for classifications, based on highest probabilities (selecting a single label out of many possible labels). \n",
    "- No activation function &mdash; for non-probability predictions. Eg. what will the temperature be tomorrow?\n",
    "\n",
    "For the MNIST digit classifier, for instance, $\\texttt{softmax}$ is the best choice. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "Consider the output layer of the MNIST digit classifier.\n",
    "\n",
    "1. Get the raw outputs (with no activation function applied to the weighted sums)\n",
    "\n",
    "<img src=\"img/softmax_1.png\" style=\"width: 75%\">\n",
    "\n",
    "2. For each output, pass it through $e^{x}$\n",
    "\n",
    "<img src=\"img/softmax_2.png\" style=\"width: 75%\">\n",
    "\n",
    "3. Get the sum of all the outputs, then divide every node's value by that sum. Now we've obtained a probability distribution for each label:\n",
    "\n",
    "<img src=\"img/softmax_3.png\" style=\"width: 75%\">\n",
    "\n",
    "\n",
    "The sum of all the final values add up to one. From this, we can see that $\\texttt{softmax}$ tends to attenuate the weaker possibilities and amplify one output. If we had $\\texttt{sigmoid}$, we would have obtained:\n",
    "\n",
    "<img src=\"img/softmax_vs_sigmoid.png\" style=\"width: 75%\">\n",
    "\n",
    "This consequently affects backpropagation because the deltas are quite large for the other 9 output nodes, even though the neural network was able to indicate which label it thought was the best answer. The network will then proceed to unnecessarily change its weights. With $\\texttt{softmax}$, the deltas for the other 9 output nodes are basically 0, so no weight update is incurred, the way it should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note: the \"input to a layer\" refers to the raw vector of values obtained from the multiplication of the previous layer and the weights to the current layer. \n",
    "\n",
    "To compute $\\texttt{layer_n_delta}$, we just multiply the backpropagated delta by the layer's slope. Eg. $\\texttt{layer_1_delta} = (\\texttt{layer_2_delta} \\times \\texttt{weights_1_2}) \\times \\texttt{reluDerivative(layer_2_delta)}$\n",
    "\n",
    "In the case of $\\texttt{sigmoid}$, looking at the slope, we can see that for large inputs, the impact of further increases to the input is much weaker. For large negative inputs, the impact is similarly weaker. This reduces how severely the weights will be adjusted, meaning that we don't run a smaller risk of corrupting their usefulness in predicting other labels. In general, nonlinearities like $\\texttt{sigmoid}$ tend to prevent the corruption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax is best used with the <em>cross-entropy</em> error function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
