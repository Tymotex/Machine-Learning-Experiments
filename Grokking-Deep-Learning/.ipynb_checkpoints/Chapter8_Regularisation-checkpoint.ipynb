{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "The goal of regularisation is the combat overfitting in neural networks. A neural network that overfits is a neural network that has learned the <em>noise</em> in the dataset rather than the <em>true</em> signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I:349 Error:0.108 Correct:1.099"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "reluDerivative = lambda x: x >= 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for j in range(iterations):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    for i in range(len(images)):\n",
    "        layer_0 = images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        error += np.sum((labels[i : i + 1] - layer_2) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(labels[i : i + 1]))\n",
    "        layer_2_delta = labels[i : i + 1] - layer_2\n",
    "        layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * reluDerivative(layer_1)\n",
    "        weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "        weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    sys.stdout.write(\"\\r\"+ \\\n",
    "                     \" I:\" + str(j)+ \\\n",
    "                     \" Error:\" + str(error / float(len(images)))[0:5] +\\\n",
    "                     \" Correct:\" + str(correct_count / float(len(images))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is done, we can test that it generalised well enough to handle the unseen dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test-Err:0.653 Test-Acc:0.7073\n"
     ]
    }
   ],
   "source": [
    "if (j % 10 == 0 or j == iterations - 1):\n",
    "    error, correct_count = (0.0, 0)\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        error += np.sum((layer_2 - test_labels[i : i + 1]) ** 2)\n",
    "        correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[i : i + 1]))\n",
    "    sys.stdout.write(\" Test-Err:\" + str(error / float(len(test_images)))[0:5] + \\\n",
    "                     \" Test-Acc:\" + str(correct_count / float(len(test_images))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<em>Test accuracy</em> &mdash; the accuracy of the predictions made by the neural network on the training dataset.\n",
    "\n",
    "### Example of Overfitting:\n",
    "The following snippet trains the neural network on varying number of iterations and logs both the train accuracy and the test accuracy. Note that this takes a while to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration: 1 | Train Error: 0.722 | Train Acc:0.537 | Test Error: 0.601 | Test Acc:0.6488\n",
      " Iteration: 10 | Train Error: 0.312 | Train Acc:0.901 | Test Error: 0.420 | Test Acc:0.8114\n",
      " Iteration: 20 | Train Error: 0.232 | Train Acc:0.946 | Test Error: 0.417 | Test Acc:0.8066\n",
      " Iteration: 30 | Train Error: 0.194 | Train Acc:0.967 | Test Error: 0.448 | Test Acc:0.7921\n",
      " Iteration: 40 | Train Error: 0.166 | Train Acc:0.984 | Test Error: 0.482 | Test Acc:0.7706\n",
      " Iteration: 50 | Train Error: 0.145 | Train Acc:0.991 | Test Error: 0.513 | Test Acc:0.7558\n",
      " Iteration: 60 | Train Error: 0.127 | Train Acc:0.998 | Test Error: 0.544 | Test Acc:0.7446\n",
      " Iteration: 70 | Train Error: 0.115 | Train Acc:0.999 | Test Error: 0.600 | Test Acc:0.723\n",
      " Iteration: 80 | Train Error: 0.108 | Train Acc:0.999 | Test Error: 0.662 | Test Acc:0.704\n",
      " Iteration: 90 | Train Error: 0.105 | Train Acc:0.998 | Test Error: 0.712 | Test Acc:0.6932\n",
      " Iteration: 100 | Train Error: 0.104 | Train Acc:0.999 | Test Error: 0.745 | Test Acc:0.6797\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "    \n",
    "relu = lambda x: (x > 0) * x\n",
    "reluDerivative = lambda x: x >= 0\n",
    "\n",
    "alpha, iterations, hidden_size, pixels_per_image, num_labels = (0.005, 350, 40, 784, 10)\n",
    "\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "for k in [1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "    for j in range(k):\n",
    "        error, correct_count = (0.0, 0)\n",
    "        for i in range(len(images)):\n",
    "            layer_0 = images[i : i + 1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            error += np.sum((labels[i : i + 1] - layer_2) ** 2)\n",
    "            correct_count += int(np.argmax(layer_2) == np.argmax(labels[i : i + 1]))\n",
    "            layer_2_delta = labels[i : i + 1] - layer_2\n",
    "            layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * reluDerivative(layer_1)\n",
    "            weights_1_2 += alpha * layer_1.T.dot(layer_2_delta)\n",
    "            weights_0_1 += alpha * layer_0.T.dot(layer_1_delta)\n",
    "    test_error, test_correct_count = (0.0, 0)\n",
    "    for i in range(len(test_images)):\n",
    "        layer_0 = test_images[i : i + 1]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        test_error += np.sum((layer_2 - test_labels[i : i + 1]) ** 2)\n",
    "        test_correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[i : i + 1]))\n",
    "    \n",
    "    print(\"\\r\"+ \\\n",
    "          \" Iteration: {}\".format(j + 1) + \\\n",
    "          \" | Train Error: \" + str(error / float(len(images)))[0:5] + \\\n",
    "          \" | Train Acc: \" + str(correct_count / float(len(images))) + \\\n",
    "          \" | Test Error: \" + str(test_error / float(len(test_images)))[0:5] + \\\n",
    "          \" | Test Acc: \" + str(test_correct_count / float(len(test_images))))\n",
    "          \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the test accuracy peaks within the first 20 or so iterations, and then declines as the number of iterations grows. This happens while training accuracy converges to 100% and the training error converges to 0.\n",
    "\n",
    "How would we get neural networks to train only on the true signal rather than overfit to the noise in the training dataset? A simple strategy is <em>early stopping</em>.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
