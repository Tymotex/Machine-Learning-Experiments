{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Deep learning is a subset of machine learning, which is a subset of artificial intelligence.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">AI subfields</th>\n",
    "        <th style=\"text-align: center;\">A neural network</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/ai-subsets.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/neural-net.png\">\n",
    "            <p style=\"text-align: left;\">\n",
    "                As a biological brain analogue, each node represents a neuron and each edge represents a synaptic connection\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "Deep learning focuses on a particular class of machine learning algorithms called neural networks &mdash; deep neural networks in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions (Nonlinearities):\n",
    "An activation function is a function associated with a given node which takes in the weighted sum (plus bias) and maps it to another number. Activation functions mimic the firing of neurons in biological neural networks. \n",
    "\n",
    "Activation functions should have the properties:\n",
    "- Continuous at every $x$ in the real numbers\n",
    "- *Injective* &mdash; one unique output for each unique input. Simple tests: check the derivative is monotonically increasing or use the horizontal line test\n",
    "- *Non-linear* &mdash; not just a straight line. The relu function is a piecewise function so it's not linear in that way. Having a non-linear function is necessary for <em>conditional correlation</em>\n",
    "- Efficient to compute &mdash; because the activation function could be called a massive number of times\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions for the Hidden Layer:\n",
    "- $\\texttt{logistic sigmoid}$: $\\frac{1}{1+e^{-x}}$ &mdash; maps weighted sums to a value in the interval $(0, 1)$\n",
    "    - __Derivative:__ suppose $z=\\frac{1}{1+e^{-x}}$, then $\\frac{dz}{dx}=z(1-z)$\n",
    "    - This lets you interpret the output of a neuron as a probability measure\n",
    "    \n",
    "<img src=\"images/sigmoid.png\" style=\"width: 35%\">\n",
    "\n",
    "- $\\texttt{tanh(x)}=\\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}=\\frac{2}{1+e^{-2x}} - 1=2 \\cdot S(2x) - 1$ &mdash; basically a scaled and offsetted version of logistic sigmoid.maps weighted sums to a value in the interval $(-1, 1)$. \n",
    "    - __Derivative:__ suppose $z=\\tanh(x)$, then $\\frac{dz}{dx}=1-z^2$ \n",
    "    - $\\texttt{tanh}$ can give a measure of negative correlation, rather than *just* positive correlation in the case of logistic sigmoid. It generally outperforms sigmoid for hidden layers because of its ability to measure negative correlation\n",
    "    - $\\texttt{tanh}$ has a steeper gradient around the neighbourhood of $x=0$ \n",
    "\n",
    "<img src=\"images/tanh.png\" style=\"width: 35%\">\n",
    "\n",
    "- $\\texttt{ReLU}=\\texttt{max(0, x)}$ &mdash; suppresses firing below 0, otherwise echoes the same input to the next layer.\n",
    "    - Computationally cheaper than the sigmoid functions.\n",
    "    - Unbounded y-values in the positive direction means the activation can 'blow-up'\n",
    "    - 'Dying ReLU problem' &mdash; for nodes that output 0, their weight cannot be adjusted during gradient descent since the gradient is 0. A substantial number of nodes in the network can become 'passive' because of that\n",
    "        - Leaky ReLU aims to mitigate this problem by ensuring nodes never have a non-zero gradient\n",
    "        \n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">ReLU</th>\n",
    "        <th style=\"text-align: center;\">Leaky ReLU</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/relu-graph.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/leaky-relu-graph.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions for the Output Layer:\n",
    "The choice of activation function in the <em>output layer</em> depends on what prediction is being made.\n",
    "- $\\texttt{sigmoid}$ &mdash; for yes/no probability predictions. \n",
    "    - Eg. is this a dog?\n",
    "- $\\texttt{softmax}$ &mdash; for classifications, based on highest probabilities (selecting a single label out of many possible labels). \n",
    "    - Eg. does this number look like a 1, a 3 or a 7? \n",
    "- No activation function for the output layer &mdash; for non-probability predictions. \n",
    "    - Eg. what will the temperature be tomorrow?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions:\n",
    "A loss function measures how closely the prediction matches the true label.\n",
    "\n",
    "-  Regression Loss Functions:\n",
    "    - __*Mean squared error*__: $E(t, z) = \\frac{1}{2}\\sum_{i=1}^{m} (t-z)^2$ &mdash; the sum of the squared difference between expected value $t$ and the forward-propagation's predicted value $z$\n",
    "        - \n",
    "\n",
    "- Binary Classification Loss Functions:\n",
    "    - __*Cross-entropy*__: $E(t, z) = -(t\\log z + (1-t)\\log (1-z))$ &mdash; where $t$ is the expected value and $z$ is the forward-propagation's prediction\n",
    "    - When $t=1$, the second term disappears, leaving $E(t, z) = -t\\log z$\n",
    "    - When $t=0$, the first term disappears, leaving $E(t, z) = -(1-t)\\log (1-z)$\n",
    "\n",
    "- Multi-Class Classification Loss Functions:\n",
    "    - __*KL-Divergence Loss*__ &mdash; TODO\n",
    "\n",
    "#### Cost Function\n",
    "Cost function: $J(w, b) = \\frac{1}{m}\\sum_{i=1}^m E(t^{(i)}, z^{(i)})$ &mdash; the average of the sum of loss values for all training samples $1 \\leq i \\leq m$, where $w$ and $b$ are the weight and bias.\n",
    "- The loss function measures how well the network performed on a single training example\n",
    "- The cost function measures how well the network's parameters $w$ and $b$ performed on the entire training set\n",
    "- Eg. if our loss function was cross-entropy, then the cost function would be: $J(w, b) = -\\frac{1}{m}\\sum_{i=1}^m \\big( t^{(i)}\\log y^{(i)} + (1-t^{(i)}) \\log (1-z^{(i)}) \\big)$.\n",
    "\n",
    "\n",
    "The 'learning' of a neural network is the process of finding $w$ and $b$ so as to minimise $J$. This is done through an optimisation algorithm such as __*gradient descent*__.\n",
    "\n",
    "\n",
    "#### Error/Cost Landscape:\n",
    "If we think of $E$ has height, then each the loss function defines an 'error landscape' on the weight space. The aim of neural network training then is just to find a configuration of weights where $E$ takes the global minimum.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">Error landscape</th>\n",
    "        <th style=\"text-align: center;\">Cost landscape</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/error-landscape.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/cost-landscape.png\">\n",
    "            <div style=\"text-align: left;\">\n",
    "                Where $w$ and $b$ are real numbers just to make this possible to visualise. Of course, $w$ would be higher dimensional. \n",
    "            </div>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "The partial derivative $\\frac{\\partial J(w, b)}{\\partial w}$ gives the slope along the $w$ axis, while $\\frac{\\partial J(w, b)}{\\partial b}$ gives the slope along the $b$ axis. When you 'nudge' $w$ a little bit, the derivative $\\frac{\\partial J(w, b)}{\\partial w}$ tells you how much $J(w, b)$ changes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "\n",
    "Gradient descent is an optimisation algorithm for finding the local minimum of a differentiable function. \n",
    "\n",
    "It works by repeatedly taking steps proportional to the negative of the gradient, until the weight converges on a global minimum:\n",
    "\n",
    "$$w_{new} = w_{old} - \\underbrace{\\eta \\frac{\\partial E}{\\partial w}}_\\text{step}$$\n",
    "\n",
    "Where $\\eta$ is the learning rate &mdash; how big of step the weight update should take, or how fast the gradient descends towards an optimal weight.\n",
    "- The subtraction of the derivative term or slope value, $\\frac{\\partial E}{\\partial w}$, will always be push the value in the 'downhill' direction\n",
    "\n",
    "- Higher learning rates can cause gradient descent to overshoot and 'bounce' up the convex error landscape. Slower learning rates cause slower training time and may cause the network to get stuck \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">Gradient Descent</th>\n",
    "        <th style=\"text-align: center;\">Fast vs. slow learning rate</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/gradient-descent-demo.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/gradient-descent-fast-vs-slow.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph\n",
    "\n",
    "Fundamentally, all neural networks are just a single big mathematical function.\n",
    "\n",
    "When trying to find a neural network that works for a particular application, you're implicitly saying 'there exists some mathematical function that will reasonably approximate the observed behaviour'. The training of neural networks is to find this approximating function.\n",
    "\n",
    "A computational graph is a way of representing math functions in graph theory.\n",
    "\n",
    "#### Representing Functions:\n",
    "\n",
    "Each node is either an input node, or a function node. Function nodes take in input and produce an output.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th style=\"text-align: center;\">Representing $f(x, y, z)=(x+y) \\cdot z$</th>\n",
    "        <th style=\"text-align: center;\">Representing $f(x, y)=ax^2+bxy+cy^2$</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/simple-computational-graph.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/another-computational-graph.png\">\n",
    "            <p style=\"text-align: left;\">\n",
    "                This could technically be considered a neural network. It's possible to train it with gradient descent and backprop and tune $a, b$ and $c$ if we had a training dataset\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "#### Representing Neural Networks:\n",
    "Even simple neural networks may have hundreds of thousands of nodes and edges in their computational graph, which would be impossible to represent in standard function notation.\n",
    "\n",
    "Each node in a neural network encapsulates smaller function nodes for: performing a weighted sum (plus bias), then computing a non-linear activation function.\n",
    "\n",
    "<img src=\"images/neural-node-computational-graph.png\" />\n",
    "\n",
    "With deep neural nets, we could have millions of individual weights, plus thousands of biases to individually tune. This is why a massive dataset and a massive amount of computing power is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The aim of back-propagation is to update the weights based on the error function value after forward-propagation. \n",
    "\n",
    "It aims to attribute the cause of the error to each node and penalise the node's output weight accordingly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an algorithm f(x), an optimization algorithm help in either minimizing or maximizing the value of f(x). In the context of deep learning, we use optimization algorithms to train the neural network by optimizing the cost function J.\n",
    "\n",
    "Gradient descent is an optimisation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Introduction:\n",
    "    - <a href=\"https://medium.com/tebs-lab/introduction-to-deep-learning-a46e92cb0022\">Intro to deep learning</a>\n",
    "- Computation graph: \n",
    "    - <a href=\"https://medium.com/tebs-lab/deep-neural-networks-as-computational-graphs-867fcaa56c9#:~:text=A%20computational%20graph%20is%20a,or%20functions%20for%20combining%20values.\">Basics of computational graphs</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
