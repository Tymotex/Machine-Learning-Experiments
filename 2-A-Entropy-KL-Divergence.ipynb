{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy:\n",
    "In statistics, thermodynamics and information theory, *entropy* is a measure of uncertainty or information (the connection between uncertainty and information is that \"an occurrence of an unlikely event gives you more *information* than the occurrence of a likely event\"). The *entropy* of a discrete probability distribution with $n$ different outcomes, $p=\\langle p_1, ..., p_n \\rangle$, is:\n",
    "\n",
    "$$H(P)=-\\sum_{i=1}^{n} p_i \\cdot \\log_{2}p_i.$$\n",
    "\n",
    "For a continuous probability distribution, $p()$, the entropy is given by:\n",
    "\n",
    "$$H(P)=-\\int_\\theta p(\\theta) \\cdot \\log p(\\theta) d\\theta.$$\n",
    "\n",
    "For a *multivariable* Normal distribution:\n",
    "\n",
    "$$H(p)=\\sum_i \\log \\sigma_i.$$\n",
    "\n",
    "#### KL-Divergence:\n",
    "\n",
    "For two probability distributions: $p=\\langle p_1, ..., p_n \\rangle$ and $q=\\langle q_1, ..., q_n \\rangle$, the *Kullback-Leibler Divergence* between $p$ and $q$ is:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\sum_{i=1}^{n} p_i (\\log_2 p_i - \\log_2 q_i).$$\n",
    "\n",
    "This gives a measure of similarity or 'distance' between two probability distributions. Note that $D_{KL}(p \\parallel q) \\neq D_{KL}(q \\parallel p)$.\n",
    "\n",
    "For continuous distributions, $p and q$, KL-divergence is given by:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\int_\\theta p(\\theta)(\\log p(\\theta) - \\log q(\\theta)) d\\theta.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Huffman Encoding:\n",
    "\n",
    "The entropy value of a frequency distribution of characters to send in a message gives the *average number of bits* required to represent a character drawn randomly from the message (in the most efficient character encoding scheme).\n",
    "\n",
    "Example 1. Suppose a message consists only of $A$ and $B$ of equal frequency. The huffman encoding would assign $A=\\texttt{0}$ and $B=\\texttt{1}$, for example. \n",
    "\n",
    "The entropy value here would be $H(\\langle 0.5, 0.5 \\rangle)=-(0.5 \\cdot \\log_2\\frac{1}{2} + 0.5 \\cdot \\log_2\\frac{1}{2}) = 1$ bit.\n",
    "\n",
    "Example 2. Suppose a message consists of $A, B \\text{ and } C$ with frequencies $0.5, 0.25 \\text{ and } 0.25$ respectively. The huffman encoding would assign $A=0, B=10, C=11$, for example.\n",
    "\n",
    "The entropy value here would be $H(\\langle 0.5, 0.25, 0.25 \\rangle)=-(0.5 \\cdot \\log_2\\frac{1}{2} + 0.25 \\cdot \\log_2\\frac{1}{4} + 0.25 \\cdot \\log_2\\frac{1}{4}) = 1.5$ bits. \n",
    "\n",
    "\n",
    "Note: $D_{KL}(q \\parallel p)$ is the number of 'extraneous' bits that would be transmitted if we designed an encoding scheme based on $q's$ frequency distribution but it turned out the samples would be drawn from $p's$ frequency distribution instead.\n",
    "\n",
    "Eg. If we designed a huffman encoding for the alphabet around the frequency distribution in English, but applied for German text, then the number of 'wastage' bits would be $D_{KL}(q \\parallel p)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward/Reverse KL-Divergence:\n",
    "Forward KL-Divergence: when we have a distribution $P$ and we want to choose a Normal distribution $Q$ which is 'close' to $P$, or 'approximately' $P$. In this case, we can use the KL-divergence, $D_{KL}(P \\parallel Q)$, as a loss function to minimise.\n",
    "\n",
    "Reverse KL-Divergence: when we have a distribution $P$ and we want to choose a Normal distribution that minimises $D_{KL}(Q \\parallel P)$ rather than $D_{KL}(P \\parallel Q)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward             |  Reverse\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/forward-kl-divergence.png)   |  ![](images/reverse-kl-divergence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations on Backpropagation:\n",
    "\n",
    "### Cross Entropy\n",
    "Cross-entropy is a measure of the relative entropy between two probability distributions over the same set of events.\n",
    "\n",
    "For classification tasks where the output should be either 0 or 1, the mean squared error loss function works poorly. \n",
    "\n",
    "Instead, we can use the cross-entropy error function $E=-(t\\log (z) + (1-t)\\log(1-z))$, where $z$ is the sigmoid function and $t$ is the target value.\n",
    "\n",
    "- If $t=1$, $E=-\\log (y)$\n",
    "- If $t=0$, $E=-\\log(1-y)$\n",
    "\n",
    "This forces the network to put higher emphasis on misclassifications \n",
    "\n",
    "Eg. In the case of detecting credit card fraud, there would be a way bigger proportion of negative instances than positive instances. Cross-entropy would place greater emphasis on the positive instances it misclassifies, which in this application, is extremely important.\n",
    "\n",
    "Choosing the cross-entropy error function makes backpropagation computations simpler. Suppose we have the logistic sigmoid activation function: $z=\\frac{1}{1+e^{-s}}$. Note how\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial z}=\\frac{z-t}{z(1-z)},$$\n",
    "and applying the chain rule, we have\n",
    "$$\\frac{\\partial E}{\\partial s}=\\frac{\\partial E}{\\partial z}\\cdot \\frac{\\partial z}{\\partial s}=z-t,$$\n",
    "\n",
    "a very simple result.\n",
    "\n",
    "<a href=\"https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735\">Good article for clarification on cross-entropy</a>\n",
    "\n",
    "#### Maximum Likelihood:\n",
    "A hypothesis is a particular set of weights.\n",
    "\n",
    "$P(D|h)= \\text{probability of data } D \\text{ being generated under hypothesis } h \\in H$, where $H$ is a class of hypotheses. Ie. $P(D|h)$ is the probability that we observe $D$, given a particular set of weights.\n",
    "\n",
    "$\\log{P(D|h)}$ is called the *likelihood*.\n",
    "\n",
    "\n",
    "We want to maximise $P(D|h)$.\n",
    "\n",
    "... TODO. What the fuck is data $D$\n",
    "\n",
    "\n",
    "### Weight Decay\n",
    "When weights 'blow up', it can inhibit the network's learning ability. For instance, when the weights are for a neuron, the output will be large which means a sigmoid activation function is virtually the same as a step function.\n",
    "\n",
    "To 'encourage' the weights to remain small, we add a *penalty* term to the loss function. \n",
    "\n",
    "$$E=\\frac{1}{2}\\sum_i (z_i-t_i)^2+\\underbrace{\\frac{\\lambda}{2}\\sum_j w_j^2}_{\\text{Penalty term}}$$\n",
    "\n",
    "Since the goal is to *minimise* the error function, by adding an additional term like $\\frac{\\lambda}{2}\\sum_j w_j^2$, ... TODO wait how does this work\n",
    "\n",
    "Where $\\lambda$ is empircally determined by fine-tuning. Eg. $\\lambda=0.00001$.\n",
    "\n",
    "\n",
    "TODO: What is Bayesian inference?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Dampens oscillations in a 'rain gutter' part of the error landscape. Adding the momentum factor amplifies the descent to the bottom by $\\frac{1}{1-\\alpha}$\n",
    "\n",
    "\n",
    "\n",
    "TODO: ... what\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
