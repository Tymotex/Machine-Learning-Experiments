{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snippets of neural network definitions and other essential building blocks in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate Structure of a PyTorch Program (Supervised Learning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch model can be deployed on a CPU or GPU\n",
    "net = MyModel().to(device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(...)\n",
    "test_loader = torch.utils.data.DataLoader(...)\n",
    "\n",
    "# Choosing an optimiser, eg. stochastic gradient descent, Adam, etc.\n",
    "optimizer = torch.optim.SGD(net.parameters,...)\n",
    "\n",
    "# Training is done in epochs\n",
    "for epoch in range(1, epochs):\n",
    "    train(params, net, device, train_loader, optimizer)\n",
    "    if epoch % 10 == 0:\n",
    "        test(params, net, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inherits from the torch.nn.Module class \n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # The structure of the network is defined here\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Run the input through the network and return the prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Custom Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the function $(x,y) \\mapsto Ax\\log (y) + By^2$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # The Parameter constructor needs to be given a torch tensor. \n",
    "        # Here we're creating a tensor of size 1 and initialise it with a random Gaussian value\n",
    "        # Since the aim is to tune this parameter, we set requires_grad=True \n",
    "        self.A = nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "        self.B = nn.Parameter(torch.randn((1), requires_grad=True))\n",
    "    def forward(self, input):\n",
    "        output = self.A * input[:,0] * torch.log(input[:,1]) + self.B * input[:,1] * input[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Net from Individual Components:\n",
    "The following network would be suitable model for the XOR multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        # From an input layer with 2 nodes to a hidden layer with 2 nodes\n",
    "        self.in_to_hid  = torch.nn.Linear(2, 2)\n",
    "        # From a hidden layer with 2 nodes to an output layer with 1 node\n",
    "        self.hid_to_out = torch.nn.Linear(2, 1)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Takes the input vector and multiplies it with the weight matrix from input layer -> hidden layer\n",
    "        hid_sum = self.in_to_hid(input)\n",
    "        \n",
    "        # Apply tanh on each component of the hidden layer's output\n",
    "        hidden  = torch.tanh(hid_sum)\n",
    "        \n",
    "        # Matrix multiplication of hidden layer output with the weights going into the output layer\n",
    "        out_sum = self.hid_to_out(hidden)\n",
    "        \n",
    "        # Applying the sigmoid function on the final output\n",
    "        output  = torch.sigmoid(out_sum)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/xor-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Sequential Network:\n",
    "Modules are added in the order that they are passed into the $\\texttt{Sequential}$ constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self, num_input, num_hidden, num_out):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(num_input, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_out),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequential Components:\n",
    "- Neural network layers:\n",
    "    - $\\texttt{nn.Linear()}$   — for linear layers\n",
    "    - $\\texttt{nn.Conv2d()}$   — for 2D convolutional layers\n",
    "   \n",
    "- Intermediate operators — these are applied *prior* to the activation function\n",
    "    - $\\texttt{nn.Dropout()}$\n",
    "    - $\\texttt{nn.BatchNorm()}$\n",
    "\n",
    "- Activation functions:\n",
    "    - $\\texttt{nn.Tanh()}$\n",
    "    - $\\texttt{nn.Sigmoid()}$\n",
    "    - $\\texttt{nn.ReLU()}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working With Data:\n",
    "The following snippet declares the input dataset for the XOR network and the expected output predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "input    = torch.Tensor([0, 0],\n",
    "                        [0, 1],\n",
    "                        [1, 0],\n",
    "                        [1, 1])\n",
    "expected = torch.Tensor([0],\n",
    "                        [1],\n",
    "                        [1],\n",
    "                        [0])\n",
    "\n",
    "# TensorDataset forms the training dataset from the input samples and corresponding target outputs\n",
    "xdata        = torch.utils.data.TensorDataset(input, expected)\n",
    "train_loader = torch.utils.data.DataLoader(xdata, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch size: \n",
    "*Batch size* is a hyperparameter of gradient descent. The $\\texttt{batch_size}$ defines the number of input datapoints to be propagated through the network, after which, the network's weights are updated. \n",
    "\n",
    "Eg. specifying a batch size of 100 will use the first 100 datapoints from the input dataset to train the network for the first training iteration. For the next iteration, it takes the next 100 datapoints from the input dataset to train the network, and so on.\n",
    "\n",
    "Training in mini-batches:\n",
    "- Requires less memory. You can't fit a massive dataset in memory all at once\n",
    "- Weights are learned more quickly since we are making updates after each batch is completed, as opposed to making 1 single update after the entire input dataset has been propagated through\n",
    "- Becomes less accurate the smaller the batch\n",
    "\n",
    "#### Epoch and Iterations:\n",
    "An *epoch* is one complete iteration through the *entire* dataset, forward and backward through the network.\n",
    "\n",
    "An *iteration* is the number of batches in 1 epoch.\n",
    "\n",
    "Since gradient descent is an iterative process, making further epochs will converge the weights closer to 0% error. Only running one epoch usually leads to underfitting. Running too many epochs will usually lead to overfitting. The number of epochs depends on the diversity of the training dataset's samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Datasets\n",
    "\n",
    "For some widely used datasets, we have special methods just for fetching them online and then loading for training. See a list of classic datasets provided by pytorch <a href=\"https://pytorch.org/docs/stable/torchvision/datasets.html\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as dsets\n",
    "\n",
    "mnist    = dsets.MNIST(...)       # Handwritten digits dataset\n",
    "cifarset = dsets.CIFAR10(...)     # Animal and vehicle images dataset\n",
    "celebset = dsets.CelebA(...)      # Celebrity pictures dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "23:46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- Cifar neural network: https://github.com/kuangliu/pytorch-cifar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor — a rank-$n$ tensor in $m$-dimensions is a mathematical object with $n$ indices and $m^n$ *components* and obeys certain transformation rules.\n",
    "\n",
    "- \"Tensor\" comes \"to stretch\" in Latin.\n",
    "- A vector *is a* tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuum Mechanics:\n",
    "In *continuum mechanics*, *stress* is a physical quantity that expresses the internal forces that neighbouring particles of a continuous material exert on each other.\n",
    "\n",
    "Consider a cube in 3D space. It can be 'stretched' in 3 separate dimensions and 'sheared' in 6 directions:\n",
    "\n",
    "<table style=\"width: 75%\">\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='images/shear-cube.png'>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='images/stress-tensor-cube.png'>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "These 9 different stresses that can be applied to the cube are organised into a *stress tensor* like this: \n",
    "$\n",
    "    \\begin{pmatrix}\n",
    "    \\sigma_{11} & \\sigma_{12} & \\sigma_{13} \\\\\n",
    "    \\sigma_{21} & \\sigma_{22} & \\sigma_{23} \\\\\n",
    "    \\sigma_{31} & \\sigma_{32} & \\sigma_{33} \\\\\n",
    "    \\end{pmatrix}\n",
    "$.\n",
    "Each row and column correspond to a physical dimension (x, y and z).\n",
    "\n",
    "<strong>*Rank*</strong> can be thought of as the amount of information you need to find a specific *component*. Formally, rank is the number of *basis vectors* required to fully specify a *component* of the tensor.\n",
    "\n",
    "For example, since we can identify any $\\sigma_{ij}$ by specifying the row and the column, we say that this tensor is rank-$2$ and $3$-dimension. Note that the number of components is given by $dim^{rank}=3^2=9$.\n",
    "\n",
    "<img src=\"images/different-rank-tensors.png\" width=\"50%\">\n",
    "\n",
    "In general, we just use index notation instead of matrix notation to specify tensors, since matrix notation breaks beyond rank-$2$.\n",
    "\n",
    "Note that a rank-$2$ tensor is not the same as a matrix. Fundamentally, a matrix is just a data structure for numbers. A tensor, on the other hand, is a data structure that *obeys certain transformation rules*.\n",
    "\n",
    "Tensors have a deeper physical significance. \n",
    "\n",
    "#### Transformation Rules:\n",
    "- A tensor is *invariant* under a change in the coordinate system. During a change to the coordinate system, the components change according to a special set of equations, but the vector itself has not been affected. Eg. think of a displacement vector between two objects in 3D space — its components will certainly change if the coordinate system is rotated, displaced, etc., but the actual displacement vector itself preserves its physical meaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- Tensors:\n",
    "    - https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
