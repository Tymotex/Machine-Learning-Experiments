{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recurrent-network.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular feed-forward networks aren't well suited to processing sequential input data. It also can't effectively learn long-range dependencies.\n",
    "- With a sequential training sample, having a fixed input layer is problematic\n",
    "- With some training samples, long-range dependencies have to be learnt which would be difficult for a deep feed-forward network to identify and generalise to\n",
    "    - Eg. Suppose we need a model to predict the next word in the sentence  \"*France is where I grew up, but I now live in Boston. I speak fluent* \\_\\_\\_\\_\\_\". Regular feed-forward architectures simply can't generalise well enough to learn this long-range dependency\n",
    "\n",
    "Recurrent neural networks on the other hand, are good architectures for processing sequential data. Audio signals and English text are examples of sequential data that recurrent neural networks are suited to processing. A recurrent architecture addresses the following:\n",
    "1. Handling variable-length sequential inputs\n",
    "2. Tracking long-range dependencies\n",
    "3. Paying attention to the order of values in the sequence\n",
    "4. Sharing parameters across the sequence (similar to what convolutional neural networks do with their kernels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks, for example, can produce one output for a variable-length sequential input, or it can produce an output for each step in the sequential data. \n",
    "<img src=\"images/recurrent-network-sequence-modeling.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular feedforward network proceeds like this: \n",
    "$$\\texttt{input} \\to \\texttt{hidden layers} \\to \\texttt{output}.$$\n",
    "It's effectively a closed system, so there's no idea of *context* or *memory of the past* that is factored into this predictive model. A recurrent neural network introduces this concept of *memory* by keeping track of what the hidden layer activations of previous timesteps were. The predictive model is then like this:\n",
    "$$\\texttt{input + previous hidden layer activations} \\to \\texttt{hidden layers} \\to \\texttt{output}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this model, the output is also deterministic given the hidden layer, but now the specific hidden layer values is only reachable *with the right sequence of inputs*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">here</a>: \"Humans don’t start their thinking from scratch every second. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence.\n",
    "\n",
    "Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>    \n",
    "            <p style=\"text-align: center;\">\n",
    "                <strong>Elman Recurrent Network</strong>\n",
    "            </p>\n",
    "            <img src=\"images/elman-recurrent-network.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">\n",
    "                <strong>Recurrent Network with shortcuts</strong>\n",
    "            </p>\n",
    "            <img src=\"images/recurent-network-shortcut.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recurrent-network-hidden-state-transfer.gif\" width=\"75%\" />\n",
    "<p style=\"text-align: center;\" width=\"75%\">The transfer of hidden layer activations across 4 timesteps. The first timestep has no context layer. The colour of the hidden layer nodes represents the memory of previous hidden layer values</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Recurrence of information:__ \n",
    "With each time step, the input follows the standard feed-forward procedure to the output layer, but the hidden layer activations are copied to the \"context\" layer, or also called the \"hidden state\", which will be then be fed as additional input as subsequent timesteps go through the feed-forward procedure.  \n",
    "    - Eg. Once `l1` is fed through the network and the output layer is computed, the intermediate hidden layer values are copied over to nodes `s1`, `s2` and `s3` as additional inputs for when `l2` is passed through the network \n",
    "    - This allows information to persist across timesteps and be factored into future predictions\n",
    "\n",
    "    \n",
    "- Sometimes shortcut connections between the input layer and output layer are added. TODO: why does is this helpful sometimes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">  \n",
    "            <p>\n",
    "                A recurrent network can be 'unrolled' into an equivalent feed-forward network. \n",
    "            </p>\n",
    "            <img src=\"images/recurrent-unrolled.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/recurrent-network-unrolled.png\">\n",
    "            <p>\n",
    "               All the weight matrices $W_{xh}, W_{hh}, W_{hy}$ are reused across each time step.\n",
    "               $W_{hh}$ transforms the current hidden state to the next hidden state. The total loss $L$ is simply the sum of all the individual losses computed at each timestep: $L_1 + L_2 + ...$\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21:00 in MIT VIDEO!!!!!!!!!!!!!!!!!!!!!!!111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time:\n",
    "*Backpropagation through time* &mdash; backpropagation on the unrolled chain of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/backprop-through-time.png\" width=\"100%\">\n",
    "            <p>\n",
    "               At each timestep, we produce a prediction $\\hat{y}_t$ which results in a loss function value $L_t$. The sum of all such loss function values gives $L = L_1 + L_2 + \\dots + L_t$. \n",
    "            </p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/backpropagation-through-time.gif\" width=\"100%\">\n",
    "            <p>\n",
    "                The black node represents the prediction, the yellow represents the error function value, the mustard colour represents the derivatives\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, there are only 3 weight matrices involved. Backpropagation through time can be considered as regular backpropagation on the unrolled network, reusing the same 3 weight matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM and GRU Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Term Memory Problem:\n",
    "<img src=\"images/short-term-memory-problem.gif\" width=\"25%\">\n",
    "Short term memory problem &mdash; caused by vanishing gradient problem. In this case, the influence of information from steps from a long time ago diminshes. Long range dependencies aren't effectively learned by normal RNN architectures\n",
    "- In the above example, the first two 'layers' for \"what\" and \"time\" are not considered much in the final prediction\n",
    "    \n",
    "LSTM, *long short-term memory*, and GRU, *gated recurrent unit* are two kinds of recurrent neural network architectures that were designed to combat the short term memory problem in normal recurrent neural networks.\n",
    "- The use of *gates* allow for better long range dependency learning. These gates are tensor operators for learning what information to add/remove to the hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs:\n",
    "A basic recurrent neural network can be thought of as a chain of modules, represented in the following diagram:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn-internal-unrolled.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An LSTM would still have the same overall structure, but it would introduce further layers acting as gates for allowing or inhibiting information flow into the *cell state*:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"70%\">            \n",
    "            <img src=\"images/lstm-internal-unrolled.png\" width=\"100%\">\n",
    "            <img src=\"images/lstm-internal-unrolled-labels.png\" width=\"75%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">Cell state of a module</p>\n",
    "            <img src=\"images/lstm-cell-state.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell state is the horizontal line running through the top of the module as shown above. It can be thought of as a conveyor belt, allowing information to flow along between each module with information sometimes being introduced via the gates.\n",
    "\n",
    "The gates are just sigmoid layers and an elementwise multiplication operation. Sigmoid is chosen because it outputs values in the range $[0, 1]$, indicating what proportion of each component should be 'let through the gate'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of the LSTM is to decide what information to forget, ie. what to exclude from the cell state. This is done by the *forget gate* which is the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Recurrent Unit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn-lstm-gru.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs train faster since they are computationally lighter than LSTM and GRU architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reber grammar, non-deterministic finite state machine, can be learnt by a simple recurrent network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">RNNs, LSTMs, GRUs</a>\n",
    "- <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMs</a>\n",
    "- <a href=\"https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\">LSTM code explanation</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini\">MIT Recurrent Neural Networks</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
