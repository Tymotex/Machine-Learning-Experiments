{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recurrent-network.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular feed-forward networks aren't well suited to processing sequential input data. It also can't effectively learn long-range dependencies.\n",
    "- With a sequential training sample, having a fixed input layer is problematic\n",
    "- With some training samples, long-range dependencies have to be learnt which would be difficult for a deep feed-forward network to identify and generalise to\n",
    "    - Eg. Suppose we need a model to predict the next word in the sentence  \"*France is where I grew up, but I now live in Boston. I speak fluent* \\_\\_\\_\\_\\_\". Regular feed-forward architectures simply can't generalise well enough to learn this long-range dependency\n",
    "\n",
    "Recurrent neural networks on the other hand, are good architectures for processing sequential data. Audio signals and English text are examples of sequential data that recurrent neural networks are suited to processing. A recurrent architecture addresses the following:\n",
    "1. Handling variable-length sequential inputs\n",
    "2. Tracking long-range dependencies\n",
    "3. Paying attention to the order of values in the sequence\n",
    "4. Sharing parameters across the sequence (similar to what convolutional neural networks do with their kernels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks, for example, can produce one output for a variable-length sequential input, or it can produce an output for each step in the sequential data. \n",
    "<img src=\"images/recurrent-network-sequence-modeling.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>    \n",
    "            <p style=\"text-align: center;\">\n",
    "                <strong>Elman Recurrent Network</strong>\n",
    "            </p>\n",
    "            <img src=\"images/elman-recurrent-network.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">\n",
    "                <strong>Recurrent Network with shortcuts</strong>\n",
    "            </p>\n",
    "            <img src=\"images/recurent-network-shortcut.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Recurrence of information:__ \n",
    "With each time step, the input follows the standard feed-forward procedure to the output layer, but the hidden layer activations are copied to the \"context\" layer, or also called the \"hidden state\", which will be then be fed as additional input as subsequent timesteps go through the feed-forward procedure.  \n",
    "    - Eg. Once `l1` is fed through the network and the output layer is computed, the intermediate hidden layer values are copied over to nodes `s1`, `s2` and `s3` as additional inputs for when `l2` is passed through the network \n",
    "\n",
    "    \n",
    "- Sometimes shortcut connections between the input layer and output layer are added. TODO: why does is this helpful sometimes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">  \n",
    "            <p>\n",
    "                A recurrent network can be 'unrolled' into an equivalent feed-forward network. \n",
    "            </p>\n",
    "            <img src=\"images/recurrent-unrolled.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/recurrent-network-unrolled.png\">\n",
    "            <p>\n",
    "               All the weight matrices $W_{xh}, W_{hh}, W_{hy}$ are reused across each time step.\n",
    "               $W_{hh}$ transforms the current hidden state to the next hidden state. The total loss $L$ is simply the sum of all the individual losses computed at each timestep: $L_1 + L_2 + ...$\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21:00 in MIT VIDEO!!!!!!!!!!!!!!!!!!!!!!!111"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Through Time:\n",
    "*Backpropagation through time* &mdash; backpropagation on the unrolled chain of network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/backprop-through-time.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Short Term Memory Problem:\n",
    "<img src=\"images/recurrent-network-text-example.png\" width=\"50%\">\n",
    "Short term memory problem &mdash; caused by vanishing gradient problem. In this case, the information from steps from a long time ago diminshes. Long range dependencies aren't effectively learned\n",
    "- In the above example, the first two 'layers' for \"what\" and \"time\" are not considered much in the final prediction\n",
    "    \n",
    "LSTM, *long short-term memory*, and GRU, *gated recurrent unit* are two recurrent neural network architectures that were created to combat the short term memory problem in normal recurrent neural networks.\n",
    "\n",
    "The use of *gates* allow for better long range dependency learning.\n",
    "\n",
    "These gates are tensor operators for learning what information to add/remove to the hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn-lstm-gru.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs train faster since they are computationally lighter than LSTM and GRU architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reber grammar, non-deterministic finite state machine, can be learnt by a simple recurrent network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">RNNs, LSTMs, GRUs</a>\n",
    "- <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMs</a>\n",
    "- <a href=\"https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\">LSTM code explanation</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=SEnXr6v2ifU&ab_channel=AlexanderAmini\">MIT Recurrent Neural Networks</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
