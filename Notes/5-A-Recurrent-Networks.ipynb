{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recurrent-network.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are good at processing sequential data.\n",
    "\n",
    "Audio signals and English text are examples of sequential data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Temporal Sequences:\n",
    "\n",
    "Up to now, we've only been processing static inputs.\n",
    "\n",
    "A temporal sequence is sequence of input values over time.\n",
    "\n",
    "#### Examples:\n",
    "- Audio signal inputs &mdash; for speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Recurrent Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>    \n",
    "            <p style=\"text-align: center;\">\n",
    "                Elman Recurrent Network\n",
    "            </p>\n",
    "            <img src=\"images/elman-recurrent-network.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">\n",
    "                Recurrent Network with shortcuts\n",
    "            </p>\n",
    "            <img src=\"images/recurent-network-shortcut.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With each time step, the hidden layer activations are copied to the \"context\" layer, or \"hidden state\". Eg. Once l1 is fed through the network and the output layer is computed, the hidden layer is copied over to nodes s1, s2 and s3 as additional inputs for when l2 is passed through the network \n",
    "    \n",
    "    \n",
    "\n",
    "- Sometimes shortcut connections between the input layer and output layer are added. TODO: why does is this helpful sometimes?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivalent Feed-Forward Equivalent:\n",
    "<img src=\"images/recurrent-unrolled.png\" width=\"50%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A recurrent network can be unrolled into an equivalent feed-forward network. \n",
    "- *Backpropagation through time* &mdash; backpropagation on the unrolled network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/recurrent-network-text-example.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The hidden state represents information from all the previous steps\n",
    "\n",
    "#### Short Term Memory Problem:\n",
    "Short term memory problem &mdash; caused by vanishing gradient problem. In this case, the information from steps from a long time ago diminshes. Long range dependencies aren't effectively learned\n",
    "- In the above example, the first two 'layers' for \"what\" and \"time\" are not considered much in the final prediction\n",
    "    \n",
    "LSTM, *long short-term memory*, and GRU, *gated recurrent unit* are two recurrent neural network architectures that were created to combat the short term memory problem in normal recurrent neural networks.\n",
    "\n",
    "The use of *gates* allow for better long range dependency learning.\n",
    "\n",
    "These gates are tensor operators for learning what information to add/remove to the hidden state.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/rnn-lstm-gru.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs train faster since they are computationally lighter than LSTM and GRU architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Order Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reber grammar, non-deterministic finite state machine, can be learnt by a simple recurrent network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\">RNNs, LSTMs, GRUs</a>\n",
    "- <a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTMs</a>\n",
    "- <a href=\"https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/\">LSTM code explanation</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
