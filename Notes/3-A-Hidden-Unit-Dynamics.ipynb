{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hidden Unit Dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Networks:\n",
    "\n",
    "<img src=\"images/encoder-network.png\" width=\"50%\">\n",
    "\n",
    "The goal of this *encoder network* is to reconstruct the same input vector in the output layer after having passing through the bottleneck hidden layer, which forces the input vector's dimension to be reduced from 5 to 3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIDDEN UNIT SPACE TODO:\n",
    "\n",
    "We're trying to take the weights of the neural network and plot them in a graphical way.\n",
    "\n",
    "Consider a 5-2-5 encoder network. From 5-2, we have 10 weights, plus 2 biases. From 2-5, we have 10 weights, plus 5 biases. That's 27 weights in this network.\n",
    "\n",
    "We can write them out as a list, but we can get a better understanding when we try to graph it.\n",
    "\n",
    "Graphing the hidden unit activations. \n",
    "\n",
    "Eg. if we have sigmoid, then the graph spans from 0-1, if we have tanh, then all hidden unit activations are bound from -1 to 1\n",
    "\n",
    "The dots represent input to hidden weights?\n",
    "\n",
    "The line is the boundary between when the linear combination (weighted sum?) is positive or negative. Lines represent the hidden to output weights.\n",
    "\n",
    "After the network is trained, each line should divide one point from the other points. The points get as far away from each other as possible.\n",
    "\n",
    "Is each axis representing the 2 hidden node activations? So with 8-3-8 encoders, there would be 8 points representing the input to hidden weights and planes that separate the points\n",
    "\n",
    "Each output node can be represented as a line in 2d HU space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Space Symmetry:\n",
    "\n",
    "- Swapping any pair of hidden nodes does not affect the overall network performance\n",
    "- Reversing the sign of all incoming and outgoing weights in any hidden node won't affect overall network performance (assuming the use of a symmetric transfer function like $\\tanh$)\n",
    "- Hidden nodes tend to do a similar job initially, then gradually specialise\n",
    "\n",
    "The first two points above gives rise to a very large number of other possible weight configurations that would all perform equally well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled Nonlinearity:\n",
    "\n",
    "- For smaller weights, each layer implements an approximately linear activation function, so multiple layers also implement an approximately linear activation function\n",
    "    - This means that the stacking of multiple layers would be almost equivalent to a single layer network (no matter how many layers we have, if all outputs are linear, the final output is nothing more than a linear function of the first layer's input)\n",
    "- For larger weights, the activation function approximates a step function which makes learning very slow - because the gradient is approximately 0, so gradient descent causes very small updates in weight\n",
    "    - Just switching from a sigmoid activation function to the ReLU activation function can make gradient descent work much faster\n",
    "- For typical weights, the activation function is close to linear, but takes advantage of a limited degree of nonlinearity\n",
    "\n",
    "<img src=\"images/tanhx-and-x.png\" width=\"50%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing/Exploding Gradient Problem: \n",
    "\n",
    "- __*Vanishing gradient problem*__ &mdash; when the gradients diminish exponentially across each layer from output to input\n",
    "    - Stacking more layers means the weight updates are concentrated on the layers closer to the output layer rather than the layers closer to the input layer.\n",
    "    - Remember in backpropagation, we rely on a product of partial differentials to compute the derivative of the error function with respect to a particular weight parameter. As we go back to earlier layers, this product involves more and more terms. If each of those terms are $<1$, then the overall product is tiny, so applying gradient descent at earlier nodes barely changes anything. \n",
    "    - The weights at the earlier layers effectively get 'stuck' and don't end up contributing much towards the network's minimisation of the error function.\n",
    "- __*Exploding gradient problem*__ &mdash; when the gradients grow exponentially across each layer from output to input\n",
    "    - By similar reasoning, if the partial differentials passed back to earlier layers are $>1$, then the gradients computed at the earlier layers will be large and will not properly converge to optimal weight values.\n",
    "\n",
    "\n",
    "#### Avoiding vanishing/exploding gradient:\n",
    "There are some standard ways for combating the vanishing/exploding gradient problem:\n",
    "- <a href=\"https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\">*Weight initialisation*</a> &mdash; a set of techniques for setting initial weight values that tend to prevent *activations* from vanishing/exploding in the feed-forward pass. This then prevents the gradients from vanishing/exploding in the backward-pass\n",
    "- <a href=\"https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\">*Batch normalisation*</a> &mdash; normalising the input vector to the layers of the neural network by scaling and centering the components to a reasonable range.\n",
    "- *Layerwise unsupervised pre-training* &mdash; involves first training an autoencoder network to achieve a more reasonable weight configuration prior to the training for the original task. This is similar to weight initialisation  \n",
    "- Using different activation functions like $\\texttt{ReLU}$ or $\\texttt{SeLU}$:\n",
    "    <table width=\"50%\">\n",
    "      <tr>\n",
    "        <td> <img src=\"images/relu.png\"></td>\n",
    "        <td><img src=\"images/selu.png\"></td>\n",
    "       </tr>\n",
    "    </table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combating Overfitting:\n",
    "The following are a few standard methods for combating overfitting in neural networks:\n",
    "- __*Early stopping*__ — stopping the training when the neural network has achieved a minimal error on the testing or validation dataset \n",
    "    <table>\n",
    "        <tr>\n",
    "            <td> \n",
    "                <img src=\"images/error-vs-weight-updates-1.png\"/>\n",
    "            </td>\n",
    "            <td>\n",
    "                <img src=\"images/error-vs-weight-updates-2.png\"/>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    Experimenting with the number of hidden nodes and number of weight updates and plotting graphs like this helps us decide a good number of training epochs to stop training at. In general, the more epochs, the greater the chance of overfitting to the training data.\n",
    "- __*Dropout*__ — randomly choosing a subset of nodes to not be used for each mini- batch. Each node is chosen to be 'dropped' from training with a fixed probability $\\pi$ you choose\n",
    "    - When the network is deployed, all nodes are active, but all the activation functions are multiplied by a factor $1 -\\pi$. This is so that the overall contribution of each node in total is equal on average to the total contribution when the network had dropped out nodes\n",
    "    - Inspired by biological neural networks where neurons may 'drop out' and other neurons have to 'fill' in for their contribution to the output\n",
    "    \n",
    "- __*Ensembling*__ — training multiple different networks on the same task and then making a prediction as a majority vote across the ensemble of networks.\n",
    "    - Bagging — each network can be trained on the same 'bag' of datapoints with replacement. For one network, some items will be trained on multiple times or none at all, and this will be different for each network, giving rise to diversity across each network \n",
    "    - Dropout is implicitly a form of ensembling because on deployment, it's like the combination of multiple different architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79\">Weight Initialisation</a>\n",
    "- <a href=\"https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/\">Batch normalisation</a>\n",
    "- <a href=\"https://machinelearningmastery.com/greedy-layer-wise-pretraining-tutorial/\">Layerwise Unsupervised Pretraining</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
