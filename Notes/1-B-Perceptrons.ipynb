{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "A *perceptron* models a biological neuron with an algorithm for the supervised learning of a binary classification of inputs. Put simply, a perceptron is just an algorithm for learning a *threshold function* — ie. a function that maps a vector $x$ to either $0$ or $1$, hence classifying that vector.\n",
    "\n",
    "- The single-layer perceptron is the simplest example of a feed-forward artificial neural network. It is only capable of learning *linearly-separable* datapoints\n",
    "<img src=\"images/linearly-separable.png\" width=\"25%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCulloch and Pitts Neuron (1943):\n",
    "McCulloch and Pitts proposed the first computational model of a biological neuron in 1943. This precedes the invention of the perceptron algorithm in 1958.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/mcculloch-pitts-neuron.png\" width=\"100%\">\n",
    "            <p style=\"text-align: center;\"><em>This is where it all began</em></p>\n",
    "            <p style=\"text-align: left;\">\n",
    "                The left side of the neuron takes in inputs and computes an aggregated value, and the right side takes in that aggregated value and outputs a decision.\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "                The input vector models the electrochemical signals a biological neuron receives through its dendrites.   >\n",
    "            <p style=\"text-align: left;\">\n",
    "                The function $g$ just directly sums the boolean inputs, plus a bias term. The function $f$ decides to output $0$ or $1$ based on whether $g(x)>\\theta$, where $\\theta$ is some threshold value.   \n",
    "            </p>\n",
    "            </p>\n",
    "        </td>\n",
    "        <td width=\"55%\">\n",
    "            <img src=\"images/mcculloch-pitts-neuron-vs-biological.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rosenblatt's Perceptron (1958):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenblatt's perceptron model extends the McCulloch-Pitts neuron model, except it takes in a vector of continuous real numbers and performs a weighted sum rather than a direct sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "Single-Layer Perceptron             |  Alternative Diagram\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/perceptron-annotated.png)  |  ![](images/perceptron-alt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Activation function* — translates the aggregated input signals to an output signal. Usually used interchangeably with the term 'transfer function'\n",
    "\n",
    "- *Bias* — a constant term that offsets the weighted sum, independent of the input. A positive bias means this neuron is more predisposed to firing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm:\n",
    "The process of 'learning' in a single-layer perceptron model is the process of adjusting the weights until it can correctly map an input to the correct output class.\n",
    "\n",
    "The perceptron makes a prediction by taking the __dot product__ of the __input vector__ and __weight vector__ and adding the bias, then passing that through an activation function, for instance, the  Heaviside step function:\n",
    "\n",
    "$$g(s) = \\begin{cases}\n",
    "            1, & s \\geq 0,\\\\\n",
    "            0, & s < 0. \\tag{1}\n",
    "         \\end{cases}$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron Learning Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a perceptron that just takes in a vector with 2 components: $x=[x_1, x_2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/perceptron-learning-rule.png\" alt=\"Rule\" style=\"width: 50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron takes in the input vector $x=[x_1, x_2]$ and has weights $w = [w_1, w_2]$ and an additional bias term $w_0$. Taking the dot product, we get $x \\cdot w =w_1 x_1 + w_2 x_2$. \n",
    "\n",
    "The aggregated sum is $s = w_1 x_1 + w_2 x_2 + w_0$. Once we pass this through the transfer function in $(1)$, $g(s)$, we get the predicted class that the input $x=[x_1, x_2]$ belongs to.\n",
    "\n",
    "If the predicted class was incorrect, then we must adjust the weights of the perceptron, $w=[w_1, w_2]$ and $w_0$, so that future predictions will be more likely to correctly classify this input.\n",
    "\n",
    "When we make updates to the weights, we use a factor term $\\eta$, called the learning rate. This affects how much the weights of the perceptron are updated by. A large learning rate allows the  \n",
    "\n",
    "- If $g(s)=0$ but the correct class was $1$, then we reassign the weights:\n",
    "    $$\n",
    "        \\begin{align}\n",
    "        w_0 &:= w_0 + \\eta \\\\\n",
    "        w_1 &:= w_1 + \\eta x_1 \\\\\n",
    "        w_2 &:= w_2 + \\eta x_2.\n",
    "        \\end{align}\n",
    "    $$\n",
    "  Now, in the future when we receive the same input, the value $g(s)$ will be larger and therefore the perceptron will more likely output $g(s)=1$, the correct class.\n",
    " \n",
    "- If $g(s)=1$ but the correct class was $0$, then we reassign the weights:\n",
    "    $$\n",
    "        \\begin{align}\n",
    "        w_0 &:= w_0 - \\eta \\\\\n",
    "        w_1 &:= w_1 - \\eta x_1 \\\\\n",
    "        w_2 &:= w_2 - \\eta x_2.\n",
    "        \\end{align}\n",
    "    $$\n",
    "  Now, in the future when we receive the same input, the value $g(s)$ will be lower and therefore the perceptron will more likely output $g(s)=0$, the correct class.\n",
    "\n",
    "- If the output class was correct, then no learning is done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Separability:\n",
    "\n",
    "The weight vector of the perceptron model with $w=[w_1, w_2]$ and which takes in inputs $x=[x_1, x_2]$ defines a straight line decision boundary with equation: $w_1x_1 + w_2x_2+w_0=0$.\n",
    "\n",
    "Eg. suppose you have a perceptron with weight vector $w=[0.5, -0.24]$ and bias $w_0=0.31$. This defines a decision boundary with the equation $0.5x-0.24y+0.31=0$.\n",
    "<img src=\"images/perceptron-linar-separability.png\" width=\"50%\">\n",
    "\n",
    "Note: if we had higher dimensional input vectors and weight vectors of size $n$, then the equation $w_0+w_1x_1+\\dots + w_nx_n=0$ defines a hyperplane surface instead.\n",
    "\n",
    "\n",
    "Rosenblatt mathematically proved that the perceptron learning algorithm guarantees that a decision boundary will be eventually 'learned', provided that the dataset is linearly separable.\n",
    "\n",
    "The perceptron learning algorithm unfortunately never learns the correct decision boundary if the learning set is not linearly separable. A dataset is not linearly separable if there exists no hyperplane that can classify all inputs into the correct class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The classic example of this is the XOR function. XOR can't be learned by a single-layer perceptron because it is not linearly separable:\n",
    "<img src=\"images/perceptron-and-or-xor.png\" width=\"50%\">\n",
    "The XOR function can be learned by a multi-layer perceptron, however.\n",
    "- **About multi-layer perceptrons:**\n",
    "    - Multi-layer perceptrons can refer generally to any feedforward neural network, that is, any network where the connections between nodes don't form a cycle such that information 'flows' forward\n",
    "    <img src=\"images/multilayer-perceptrons.png\" width=\"50%\">\n",
    "    - With multi-layer perceptrons containing a *hidden layer*, we need a more sophisticated training method called *backpropagation* for updating the weights of the network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/mcculloch-pitts-model-5fdf65ac5dd1\">McCulloch and Pitts Neuron Model</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
