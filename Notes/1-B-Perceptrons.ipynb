{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons\n",
    "\n",
    "A perceptron models a biological neuron with an algorithm for the supervised learning of a binary classification pattern. Put simply, a perceptron is an algorithm for learning a threshold function — a function that maps a vector x to either 0 or 1, hence classifying that vector  \n",
    "\n",
    "- Single-layer perceptrons are only capable of learning *linearly-separable* patterns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCulloch and Pitts Neuron:\n",
    "This single-layer perceptron is the simplest feedforward neural network.\n",
    "\n",
    "\n",
    "Single-Layer Perceptron             |  Alternative Diagram\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/perceptron-annotated.png)  |  ![](images/perceptron-alt.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Transfer function* or *activation function* — translates the input signals to an output signal. Usually used interchangeably with the term 'activation function'\n",
    "\n",
    "- *Bias* — a positive number means this neuron is more predisposed to firing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron Learning Algorithm:\n",
    "The process of 'learning' in a single-layer perceptron is the process of adjusting the weights until the correct output.\n",
    "\n",
    "A prediction is made by taking the dot product of the input vector and weight vector and adding the bias, then passing that through an activation function (eg. the Heaviside step function).\n",
    "\n",
    "\n",
    "<img src=\"images/perceptron-learning-rule.png\" alt=\"Rule\" style=\"width: 50%;\"/>\n",
    "\n",
    "#### Possible Cases After Prediction:\n",
    "1. If the prediction was 0 when it should have been 1: then we need <em>higher weights</em> for the edges corresponding to <em>higher input values</em>, which is why we add $\\eta x_k$ to each weight.\n",
    "2. If the prediction was 1 when it should have been 0: then we need <em>lower weights</em> for the edges corresponding to <em>higher input values</em>, which is why we subtract $\\eta x_k$ from each weight.\n",
    "3. If the prediction was correct: the weights are left alone. No learning happens in this case\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial setup:\n",
    "Initially, the weights are small random values.\n",
    "\n",
    "$\\eta$ — learning rate. Takes value between 0 and 1. Determines the magnitude of weight changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Separability:\n",
    "\n",
    "Rosenblatt proved mathematically that this algorithm guarantees that a decision boundary will be 'learned', provided that the data it works with is linearly separable.\n",
    "\n",
    "The algorithm never learns the correct decision boundary if the learning set is not linearly separable. Ie. the input vectors that are positive and the input vectors that are negative cannot all be separated by a single hyperplane.\n",
    "\n",
    "- The classic example of this is the XOR function. XOR can't be learned by a single-layer perceptron, but it can be learned by a multi-layer perceptron. The development of neural networks was mostly abandoned in the 1960s before a resurgence in the 1980s.\n",
    "- **About multi-layer perceptrons:**\n",
    "    - Multi-layer perceptrons can refer generally to any feedforward neural network, that is, any network where the connections between nodes don't form a cycle such that information 'flows' forward\n",
    "    - With multi-layer perceptrons containing a hidden layer, we need to use backpropagation to be able to attribute error to specific weights\n",
    "    -"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$g(s) = \\begin{cases}\n",
    "            1, & s \\geq 0,\\\\\n",
    "            0, & s < 0.\n",
    "         \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
