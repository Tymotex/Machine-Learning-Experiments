{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Language Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting Frequencies:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/document-classification-matrix.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/there-was-a-crooked-man.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting word frequencies are useful for document classification. The above table can be turned into a matrix where rows correspond to a single word and columns correspond to a document. The values in the cell represent the frequency of a word in a particular document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Gram Model &mdash; Counting Consecutive Word Pair Frequencies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>  \n",
    "            <p style=\"text-align: center;\">\n",
    "                Consecutive Word Pair Frequency Matrix\n",
    "            </p>          \n",
    "            <img src=\"images/consecutive-word-frequency.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">\n",
    "                Bigram Model\n",
    "            </p>\n",
    "            <img src=\"images/1-gram-consecutive-word-frequency.png\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can turn the consecutive word pair frequency matrix into a matrix of probability distributions like the bigram model. In the bigram model, every cell value is normalised by dividing it by the sum of all the values in the row. With this, we can estimate the probability of word $w_j$ appearing after word $w_i$. Eg. the probability of \"crooked\" following \"a\" is $\\frac{6}{7}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __N-gram__ &mdash; is a contiguous sequence of N words. The N-gram model has $N$ dimensions and is used to store the probability of different sequences of $N$ consecutive words occurring together in a document.\n",
    "    - It's necessary to aggregate statistics across a large collection of similar documents to prevent unusual words like \"crooked\" from dominating the probability distribution\n",
    "    - A bigram model using the English vocabulary would typically be of size $60000 \\times 60000$\n",
    "    - An N-gram model would be of size $\\underbrace{60000 \\times 60000 \\times \\ldots \\times 60000}_{N times}$, taking exponentially more RAM as $N$ increases\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence Matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <p style=\"text-align: center;\">2-Word-Window Co-Occurrence Matrix</p>\n",
    "            <img src=\"images/co-occurrence-matrix-2-window.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\">10-Word-Window Co-Occurrence Matrix</p>\n",
    "            <img src=\"images/co-occurrence-matrix-10-window.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *co-occurrence matrix* stores the probability of a word occurring near another word instead of focusing specifically on whether $w_j$ appears after $w_i$ like for the N-gram model.\n",
    "\n",
    "A window size of $2$ would count 1 word on either side of the current word. A window size of $10$ would count 5 words on either side of the current word.\n",
    "\n",
    "Each word's row can be regarded as the vector representation of that word. With the co-occurrence matrix, words that appear in similar contexts tend to have similar vector representations.\n",
    "\n",
    "The input vector dimensions can be on the order of $10^5$ because of the vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Word Embeddings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of *word embeddings* is to find a vector representation for each word where 'nearby' representations are likely to occur in similar contexts. *Word embeddings* are word vectors.\n",
    "\n",
    "A vector representation for \"apple\" might look like $[0.7, 1.2, -0.345, ...]$. This word embedding may consist of any number of elements, this is a parameter that we decide. An interesting consequence of numericalising words with a technique like *Word2Vec* is that the Euclidean distance between any 2 word vectors represents similarity. So vector operations can give you results like $\\texttt{apple + purple} \\approx \\texttt{plum}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition:\n",
    "Singular value decomposition is frequently used for *dimensionality reduction*.\n",
    "\n",
    "In linear algebra, the *singular value decomposition* is a factorisation of any matrix $X=USV^T$ where $U_{L\\times L}$ and $V_{M \\times M}$ are square *unitary* matrices and $S_{L \\times M}$ is a *diagonal* matrix with non-negative entries $s_1, s_2, ..., s_M \\geq 0$.\n",
    "- A square matrix $A$ is unitary if the inverse is equal to the conjugate transpose: $A^{-1}=(\\overline {A})^T$\n",
    "    - The sum of the squares of the values in each row is equal to 1\n",
    "    - All rows are orthogonal to each other\n",
    "- In the case of a co-occurrence matrix $X$, the dimensions are $L\\times L$, where $L$ is the size of the lexicon\n",
    "\n",
    "\n",
    "The goal is to obtain an __approximation__ for matrix $X$ of a much __smaller rank__ $N<M$ by:\n",
    "- Truncating $U_{L \\times L}$ to $\\widetilde{U}_{L \\times N}$ a matrix with far fewer columns\n",
    "- Truncating $S_{L \\times M}$ to $\\widetilde{S}_{N \\times N}$ a matrix with far fewer columns and rows\n",
    "- Truncating $V^T_{M \\times M}$ to $\\widetilde{V}^T_{N \\times M}$, a matrix with far fewer rows\n",
    "\n",
    "The product $\\widetilde{U}_{L \\times N} \\cdot \\widetilde{S}_{N \\times N} \\cdot \\widetilde{V}^T_{N \\times M}$ is a matrix of size $L \\times M$, the same dimensions as the original matrix $X$. \n",
    "\n",
    "The $k^{th}$ row of $\\widetilde{U}$ gives the vector representing word $k$ of the lexicon.\n",
    "\n",
    "<img src=\"images/singlar-value-decomposition.png\" width=\"75%\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">\n",
    "            <p style=\"text-align: center;\">10-Word-Window Co-Occurrence Matrix</p>\n",
    "            <img src=\"images/co-occurrence-matrix-10-window.png\">\n",
    "        </td>\n",
    "        <td>            \n",
    "            <p style=\"text-align: center;\">Word vectors of size 2 obtained after running SVD on the co-occurrence matrix</p>\n",
    "            <img src=\"images/singlar-value-decomposition-example.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time complexity of SVD is $O(L\\times M^2)$, which is impractical if a large lexicon size like $L=60000$ is used. Two alternative ways to generate word vectors are *Word2Vec* and *GloVe*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/word2vec-1-word-context-model.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a word, this network predicts what other words in the lexicon are likely to appear next to that word.\n",
    "\n",
    "- The inputs are 1-hot encoded.\n",
    "- There are no activations. This hidden layer values are a purely linear function of the input layer, and the output layer values are a purely linear function of the hidden layer.\n",
    "\n",
    "- The $k^{th}$ word is represented by the $k^{th}$ row in the matrix $W_{V\\times N}$, which we'll call $v_k$. Note that $V$ is the size of the lexicon and $N$ is the number of hidden nodes (the dimension we want to compress to).\n",
    "\n",
    "- The $j^{th}$ column of matrix $W'_{N\\times V}$ is an *alternative* vector representation of the $j^{th}$ word, which we'll call $v'_j$.\n",
    "\n",
    "- The output probability for a particular word $u_j$ being nearby word $k$ is given by: $u_j=v'^T_j v_k$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec replacing SVD:\n",
    "\n",
    "With Word2Vec, the input-to-hidden weight matrix takes the place of $\\widetilde{U}$ and the hidden-to-output weights takes the place of $\\widetilde{V}^T$ in SVD. \n",
    "\n",
    "Instead of carrying out regular SVD, we're trying to learn the values of $\\widetilde{U}$ and $\\widetilde{V}$ with a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost Function:\n",
    "\n",
    "Softmax can be used to normalise the output layer values into a probability distribution.\n",
    "\n",
    "The probabilty of word $j$ occurring nearby word $k$ is given by: \n",
    "\n",
    "$$prob(j|k) = \\frac{e^{u_j}}{\\sum_{i=i}^V e^{u_i}}.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the dictionary is numbered from 0-60000 and each word in any document can be mapped to a specific number. Each document can then be considered as just a sequence of numbers: $w_1, w_2, ..., w_T$ where $w_i=j$ with $j$ being the $j^{th}$ word in the dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the network to learn weights that would maximise: \n",
    "\n",
    "$$\n",
    "    \\frac{1}{T}\\sum_{t=1}^T \\big(\\sum_{-c\\leq r \\leq c, r \\neq 0} \\log \\text{prob}(w_{t+r}|w_t)\\big)\n",
    "$$\n",
    "where $c$ is the number of words we want to look at before and after the middle word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with vanilla softmax, if the true output value is $j^*$, then the cost function is:\n",
    "\n",
    "$$\n",
    "    E = -u_{j^*} + \\log{\\sum_{i=1}^V e^{u_{i}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient for each output $u_j$ is given by \n",
    "$$e_j=\\frac{\\partial E}{\\partial u_j}=\\delta_{jj^*} + \\frac{\\partial}{\\partial u_j} \\log{\\sum_{i=1}^V e^{u_i}},$$\n",
    "where $\\delta_{jj^*} = 1$ if the prediction equals the true value $j=j^*$, otherwise $\\delta_{jj^*}=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient for hidden-to-output weights:\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial w'_{ij}} = \\frac{\\partial E}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial w'_{ij}} = e_j h_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient for hidden unit activations:\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial h_i} = \\sum_{j=1}^V \\big( \\frac{\\partial E}{\\partial u_j} \\cdot \\frac{\\partial u_j}{\\partial h_i} \\big) = \\sum_{j=1}^V \\big( e_j w'_{ij} \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The gradient for input-to-hidden weights:\n",
    "$$\n",
    "    \\frac{\\partial E}{\\partial w_{ki}} = \\frac{\\partial E}{\\partial h_i} \\cdot \\frac{\\partial h_i}{\\partial w_{ki}} = \\sum_{j=1}^V \\big( e_j w'_{ij x_k} \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: having to compute softmax on an output layer of size 60000 for every word in the text is computationally expensive. Alternatives to softmax are:\n",
    "- Hierarchical softmax\n",
    "- Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Softmax:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at the completion of training, we only keep the input-to-hidden weights and discard the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each output corresponds to one branch node in the tree.\n",
    "\n",
    "You only calculate outputs you need.\n",
    "\n",
    "The sigmoid output tells you to go left or right? The sigmoid tells you a probability.\n",
    "\n",
    "You only need to calculate $\\log_2 60000 \\approx 16$ nodes on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fuck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Fuck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue off 1:00:00ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous Bag of Words and Skip-Gram:\n",
    "\n",
    "TODO: What the fuck is this even\n",
    "\n",
    "Apaprently it's 2 different versions of word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <p style=\"text-align: center;\"><strong>Continuous Bag of Words</strong></p>\n",
    "            <img src=\"images/continuous-bag-of-words.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\"><strong>Skip-Gram</strong></p>\n",
    "            <img src=\"images/skip-gram.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Continuous bag of words* &mdash; given 10 surrounding words as input, predict the word in the middle of those 10 words\n",
    "- *Skip-gram* &mdash; given the centre word as input, predict the 10 surrounding words\n",
    "- Continuous bag of words and skip gram produce more or less the same results. Which one is used depends on which one is faster in the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <a href=\"https://gist.github.com/aparrish/2f562e3737544cf29aaf1af30362f469\">Understanding word vectors</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
