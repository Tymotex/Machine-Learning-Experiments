{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/agent-environment.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: center;\"><strong>Markov Decision Process</strong></p>\n",
    "            <img src=\"images/markov-decision-process.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We model reinforcement learning situations using a mathematical framework called the *Markov Decision Process*.\n",
    "\n",
    "In reinforcement learning with the *Markov Decision Process*, we have an *agent* which interacts with its *environment* to transition between different *states* and obtain *rewards* for transitioning to particular states. The interaction between the agent and the environment is considered in a sequence of time steps $t=1$, $t=2$, and so on.\n",
    "\n",
    "The environment defines a set $S$ of *states* and a set $A$ of actions.\n",
    "\n",
    "- At each time step $t$, the agent is in some state $s_t$ where it must choose an action $a_t$ to receive reward $r_t=R(s_t, a_t)$ transition to the next state $s_{t+1} = \\delta(s_t, a_t)$. The agent starts in state $s_0$\n",
    "    - $\\delta$ &mdash; the state transition function which maps a state and action to a state: $\\delta : S \\times A \\to S$\n",
    "    - $R$ &mdash; is the reward function that maps a state and action to a reward value: $R: S \\times A \\to \\mathbb{R}$\n",
    "        - Both $\\delta$ and $R$ are defined by the *environment*\n",
    "    - The agent has a *policy* function, $\\pi: S \\to A$, that maps states into actions\n",
    "\n",
    "\n",
    "Functions $\\delta$, $R$ and $\\pi$ can each be *stochastic* rather than *determinstic*, that is, randomly decided in some way (eg. in a game with dice rolling mechanics). In this case, we write each function as probability distributions\n",
    "- $\\delta(s_{t+1}=s|s_t, a_t)$ &mdash; the probability that next state $s_{t+1}$ is the state $s$\n",
    "- $R(r_t=r|s_t, a_t)$ &mdash; the probability that next reward $r_{t+1}$ is the reward value $r$\n",
    "- $\\pi(a_t=a|s_t)$ &mdash; the probability that next action taken $a_{t+1}$ is the state $a$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of reinforcement learning is to find a sequence of actions $a_1, a_2, \\ldots$ which maximises the expected reward.\n",
    "\n",
    "There are a few approaches to this:\n",
    "- *Value-function learning* &mdash; optimising the value function $V^\\pi$. The agent decides which state to transition to each time step based on which state has the greatest $V^\\pi$ value\n",
    "    - *Temporal Difference Learning* &mdash; \n",
    "    - *Q-Learning* &mdash;\n",
    "- *Policy learning* &mdash; determining the optimal policy $\\pi$ that maximises the cumulative reward, rather than using a value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models of Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 3 standard ways for defining the optimality of short-term rewards vs long-term rewards. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Finite horizon reward* &mdash; when you set a 'stopping point' or 'horizon' sometime in the future and you only consider the sum of rewards *up to that point and not any further*\n",
    "    - Simpler computation, but could result in poorer choices\n",
    "$$\\texttt{total reward} = \\sum_{i=0}^{h-1} \\big( r_{t + i} \\big)$$\n",
    "\n",
    "- *Infinite discounted reward* &mdash; summing rewards all the way in the future, but applying a *discount factor* $\\gamma$. The further in the future a reward is, the less emphasis it has on our next decision\n",
    "    - This optimality model is used most frequently\n",
    "$$\\texttt{total reward} = \\sum_{i=0}^{\\infty} \\big( \\gamma^i r_{t+i} \\big), \\text{ where } 0 \\leq \\gamma < 1$$\n",
    "\n",
    "- *Average reward* &mdash; we pick some number $h$, add up all the rewards up to $h$, then take the average, and observe what the value is as $h$ approaches infinity\n",
    "$$\\texttt{total reward} = \\lim_{h \\to \\infty} \\big( \\frac{1}{h} \\sum_{i=0}^{h-1} r_{t+i} \\big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">    \n",
    "            <p>\n",
    "                Here is an <em>environment</em> which models a finite state machine. Following paths $a_1, a_2$ or $a_3$ leads to different reward loops. The best path is $a_3$\n",
    "            </p>\n",
    "            <img src=\"images/models-of-optimality-comparison.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>\n",
    "                Finite horizon with $k=4$ &mdash; $a_1$ is preferred\n",
    "            </p>\n",
    "            <p>\n",
    "                Infinite discounted reward with $\\gamma=0.9$ &mdash; $a_2$ is preferred \n",
    "            </p>\n",
    "            <p>\n",
    "                Average reward &mdash; $a_3$ is preferred \n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of an agent is to maximise its *cumulative reward*.\n",
    "\n",
    "The *value* $V(s)$ of a state $s$ is a measurement of the *maximum future rewards* from being in state $s$. Decisions made by the agent shouldn't always greedily aim for highest immediate rewards but instead aim for entering the next state which has the highest *value*.\n",
    "\n",
    "In essence, the value function gives a quantitative measure of how *good it is to reach certain states*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tic-tac-toe-value-states.png\" width=\"25%\">\n",
    "<em><p style=\"text-align: center;\">The agent picks the next state based on the highest value available</p></em>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose an agent is in state $s$ and chooses actions based on policy $\\pi$. Every policy $\\pi$ determines a *value function* that maps states to numbers, \n",
    "\n",
    "$$V^{\\pi}:S\\to \\mathbb{R},$$ \n",
    "\n",
    "with the value of a particular state being: \n",
    "\n",
    "$$V^{\\pi}(s) = \\text{average discounted reward received by the agent, if they start in state } s.$$\n",
    "\n",
    "- Every value function is conditional on a policy. We have the $\\pi$ subscript in $V^\\pi$ to stress that fact.\n",
    "- The value $V^\\pi(s)$ is the expected value of being in state $s$ if they follow policy $\\pi$ forever\n",
    "<img src=\"images/value-function-expression.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an optimal policy function, $\\pi = \\pi^*$, the value $V^*(s)$ is the *maximum discounted reward* achievable at state $s$. The value $V^\\pi(s)$ is an estimate of the 'true' value $V^*(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of *value function learning* is to start with a random value function $V^\\pi$ initially, then iteratively improve it, trying to converge towards $V^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/env-policy-value-game.png\" width=\"100%\">\n",
    "<em>\n",
    "    <p style=\"text-align: center;\">\n",
    "        (a) is the environment, (b) is the policy, (c) is the value function $V^\\pi$ defined by the policy $\\pi$\n",
    "    </p>\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Tradeoff:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure convergence to the optimal strategy, the agent must try some *exploration* on unknown states, that is, risking a choice that could have poorer rewards to discover routes to better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exploitation* is sticking to choosing states that the agent *currently knows* will produce the best rewards, however it may be sticking to a suboptimal choice because the optimal choice may still be undiscovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a good balance between exploring and exploiting, and by playing infinitely many games, the value for every state will approach its true probability. \n",
    "- This good balance between exploring and exploit can be determined by the *epsilon greedy parameter*. Epsilon-Greedy is a simple method for determining epsilon $\\epsilon$, the probability of choosing to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two methods for deciding between exploration/exploitation:\n",
    "- Choosing a random action 5% of the time (epsilon-greedy strategy):\n",
    "<img src=\"images/epsilon-greedy.png\" width=\"25%\">\n",
    "\n",
    "\n",
    "- Using softmax/Boltzmann distribution to systematically choose the next action $a$:\n",
    "$$\n",
    "    P(a)=\\frac{e^{\\frac{R(a)}{T}}}{\\sum_{b \\in A} e^{\\frac{R(b)}{T}}}  \\tag{Probability of choosing action a}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example value function calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/delayed-reinforcement-example.png\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the agent has no familiarity with the environment and no idea of what the rewards for each state transition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an optimal policy that maps $\\pi(s_2)=a_1$ $\\pi(s_3)=a_2$. If $\\gamma = 0.9$, then the value of state $s_3$ is given by:\n",
    "\n",
    "$$V^\\pi(s_3)=2+0.9\\cdot 2 + 0.9^2 \\cdot 2 + ... = \\frac{2}{1-0.9}= 20.$$\n",
    "\n",
    "The value of state $s_2$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    V^\\pi(s_2) &=-1+0.9\\cdot 2 + 0.9^2 \\cdot 2 + \\ldots \\\\\n",
    "               &= -1 + 0.9(\\frac{2}{1-0.9}) = -1 + 0.9\\cdot V^\\pi (s_3) \\\\\n",
    "               &= 17.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The value of state $s_1$ is given by $V^\\pi(s_1) = -1 + 0.9 \\cdot V^\\pi (s_2) = 14.3$.\n",
    "\n",
    "What if the policy mapped $\\pi(s_1) = a_1$ instead? The value of state $s_1$ would be $V^\\pi(s_1) = 1 + 0.9 \\cdot 1 + 0.9^2 \\cdot 1 + ... = \\frac{1}{1-0.9}=10.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 classical reinforcement learning algorithms: *temporal difference learning* and *Q-learning*.\n",
    "- *Temporal difference learning* tries to learn the optimal value function $V^*$\n",
    "- *Q-learning* tries to learn the optimal Q-function $Q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning (TD Learning):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporal difference learning is when an agent learns from an environment through episodes with no prior knowledge of the environment. Temporal difference is a model-free reinforcement learning method.\n",
    "- Model-free &mdash; without using a *Markov decision process* model of the environments transition and reward functions\n",
    "\n",
    "There are multiple TD methods: $\\texttt{TD}(0)$, $\\texttt{TD}(1)$, $\\texttt{TD}(\\gamma)$\n",
    "- $\\texttt{TD}(0)$ &mdash; the simplest, tabular approach\n",
    "\n",
    "Temporal difference applies *Monte Carlo methods* and *dynamic programming*. TD methods involve *estimating the rewards at each time step*.\n",
    "- <a href=\"https://en.wikipedia.org/wiki/Monte_Carlo_method\">Monte Carlo methods</a> &mdash; a large class of algorithms involving repeated random sampling to obtain numeric results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD-Learning Update Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose $R$ and $\\delta$ are deterministic. The update rule is:\n",
    "$$\n",
    "    V(s_t) := r_t + \\gamma V(s_{t+1})\n",
    "$$\n",
    "- Suppose $R$ and $\\delta$ are stochastic instead. We have to apply this update rule instead, introducing the learning rate $\\eta$:\n",
    "$$\n",
    "    V(s_t) := V(s_t) + \\eta \\big( r_t + \\gamma V(s_{t+1}) - V(s_t) \\big)   \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo Approach vs. TD Learning Approach:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo approach to reinforcement learning involves collecting the rewards at the __end of the episode and then calculating the maximum expected future reward__ for each state taken, $V(s_t)$.\n",
    "\n",
    "The $\\texttt{TD}(0)$ learning approach doesn't wait until the end of the episode to update the maximum expected rewards for each state. Instead, we update $V^\\pi(s_t)$ at every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>       \n",
    "            <strong><p style=\"text-align: center;\">Monte Carlo update rule</p></strong>\n",
    "            <img src=\"images/monte-carlo-update-rule.png\" width=\"100%\">\n",
    "            <p style=\"text-align: center;\"><em>Where $G(t)$ is the total discounted reward accumulated</em></p>\n",
    "        </td>\n",
    "        <td>\n",
    "            <strong><p style=\"text-align: center;\">TD-learning update rule (same as equation $(1)$ above)</p></strong>\n",
    "            <img src=\"images/td-learning-update-rule.png\" width=\"100%\">\n",
    "            <p style=\"text-align: center;\"><em>Where $R_{t+1} + \\gamma \\cdot V(s_{t+1})$ is the TD-target &mdash; the estimation of what $V(s_t)$ should be</em></p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a reinforcement learning algorithm that learns the quality of actions at every state $s \\in S$. The goal of Q-learning is to find the optimal policy by learning the optimal *Q-value table*.\n",
    "\n",
    "The 'Q' in Q-learning means *quality of action*. Q-learning is a model-free reinforcement learning method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The goal of the traditional Q-learning algorithm is to build up a Q-table. The Q-table is just a lookup table that records the *maximum expected future reward* for each action at each state. The rows of the Q-table are states, the columns are actions that can be taken at those states. Each cell holds the Q-value: $Q(s, a)$\n",
    "    - Example of an environment and its corresponding Q-table \n",
    "    <img src=\"images/q-table-example.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We don't implement a policy in Q-learning. We just improve our Q-table and then our 'policy' is just to always pick the best action at each state, according to the Q-table. Think of the Q-table as a cheatsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-function is similar to the value function, but it maps a state *and an action* to a reward: $Q^\\pi : S \\times A \\to \\mathbb{R}$.\n",
    "\n",
    "- The output Q-value, $Q^\\pi(s, a)$, is the average discounted reward received by the agent when they begin in state $s$ having chosen action $a$ at the __first timestep__ and then choosing actions based on the policy $\\pi$ thereafter.\n",
    "- $Q^\\pi(s, a)$ is the expected value of first taking action $a$ from state $s$, then following policy $\\pi$ forever. \n",
    "    - This contrasts with $V^\\pi(s)$ which is the expected value of being in state $s$, following policy $\\pi$ forever.\n",
    "- Every $Q$ is conditional on a policy $\\pi$. We have superscript $\\pi$ in $Q^\\pi$ to stress this fact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/q-function-formula.png\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-function is learnt by applying the following Q-learning algorithm which uses the Bellman equation as the update rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning algorithm\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"20%\">            \n",
    "            <img src=\"images/q-learning-algorithm.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/q-learning-algorithm-pseudocode.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/q-learning-algorithm.gif\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Initialise all cells of the Q-table to 0 &mdash; we start knowing absolutely nothing about the environment\n",
    "\n",
    "Step 3: Choosing an action $a$ for this time step $t$\n",
    "\n",
    "- At the start, we initialise an exploration rate $\\epsilon=1$ &mdash; since initially everything is unexplored, therefore there are no reward pathways we could exploit\n",
    "\n",
    "- We generate a random number in the interval $[0, 1]$, and if that number is greater than $\\epsilon$, then we favour exploitation rather than exploration\n",
    "    - Initially, we must have a big epsilon value $\\epsilon$ that gradually gets smaller. Ie. as the algorithm runs longer and populates the table, the agent becomes *greedier* \n",
    "    <img src=\"images/q-learning-algorithm-epsilon-tradeoff.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps 4 and 5: Evaluating the chosen action $a$\n",
    "\n",
    "- The resulting state after choosing action $a$ is $s'$ and the reward obtained for the transition $\\delta(s, a)=s'$ is $R(s, a) = r$.\n",
    "\n",
    "- We update the cell's $Q(s, a)$ value using the Bellman equation:\n",
    "\n",
    "$$\n",
    "    Q(s, a) := Q(s, a) + \\eta \\big( r + \\gamma \\max Q'(s', a') - Q(s, a) \\big)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Function Update Rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bellman-equation.png\" width=\"100%\">\n",
    "<p style=\"text-align: center;\"><em>Alternative annotations of the Bellman equation</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perhaps a more intuitive form of the update rule:\n",
    "$$\n",
    "    Q(s_t, a_t) := \\underbrace{(1-\\alpha)Q(s_t, a_t)}_{\\text{Retained old value}} + \\alpha \\big(r_t + \\gamma \\max_a Q(s_{t+1}, a)\\big).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the Update Rule:\n",
    "\n",
    "For a deterministic environment, the optimal policy $\\pi^*$, optimal Q-function $Q^*$ and optimal value function $V^*$ are related by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pi^*(s) &= \\max_a \\big( Q^*(s, a) \\big) \\tag{1} \\\\\n",
    "    Q^*(s, a) &= \\underbrace{R(s, a)}_{\\text{immediate reward}} + \\gamma V^*\\big( \\underbrace{\\delta (s, a) \\big)}_{\\text{new state}} \\tag{2} \\\\ \n",
    "    V^*(s) &= \\max_b\\big( Q^*(s, b)\\big) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(1)$ &mdash; the optimal policy chooses the action at state $s$ to be whichever action corresponds to the highest Q-value\n",
    "\n",
    "$(2)$ &mdash; the quality of taking action $a$ at state $s$ is the immediate reward plus the discounted value of the next state\n",
    "\n",
    "$(3)$ &mdash; the value of a state $s$ is the best Q-value at that state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining $(2)$ and $(3)$, we get the equation relating $Q^*$ back to itself:\n",
    "\n",
    "$$\n",
    "    Q^*(s, a) = R(s, a) + \\gamma \\max_b \\big( Q^*\\big(\\delta(s, a), b\\big) \\big),\n",
    "$$\n",
    "\n",
    "where $b$ is the action that maximises the Q-value of the next state. The update rule in a deterministic environment follows from the above result:\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) := r_t + \\gamma \\max_b \\big( Q(s_{t+1}, b) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the environment is stochastic, then we introduce a learning rate and keep a running sum in the update rule:\n",
    "$$\n",
    "    Q(s_t, a_t) := Q(s_t, a_t) + \\eta \\big( r_t + \\gamma \\max_{b} \\big( Q(s_{t+1}, b) - Q(s_t, a_t) \\big) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Q-Value Calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/delayed-reinforcement-example.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: left;\">\n",
    "    $Q(s_2, a_2)$ is the value of state $s_2$, given that we were forced to choose $a_2$ at the first timestep.\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "                $Q^*(s_2, a_2) = 1 + 0.9\\cdot V^*(s_1) = 1+0.9\\cdot 14.3 = 13.87$.\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "                $Q^*(s_2, a_1) = V^*(s_2) = 17$, since if we were forced to pick $a_1$ at $s_2$, that would've been the same as what the optimal policy would have picked as well.\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "                $Q^*(s_3, a_1) = 1 + 0.9V^*(s_2) = 16.3$.\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoretical Results and the Introduction of Neural Networks to Reinforcement Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Theorem(. Q-learning will eventually converge to the optimal policy for any finite Markov decision process, given infinite exploration time and a partly random policy.\n",
    "- By a similar theorem, TD-learning will also theoretically converge on the optimal value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumptions upon which the theoretical results are derived presents impracticalities during training:\n",
    "- Delayed reinforcement &mdash; the discovery of better state transitions could take an unreasonably large number of timesteps\n",
    "- Search space must be finite &mdash; otherwise training becomes uselessly slow\n",
    "- Can't rely on lookup table for $Q$ values in most practical settings. Building up a Q table for *every state* and *every action possible at those states* is impossible to scale up to an environment like chess where there is an unimaginably large set of possible states\n",
    "    -  Instead, we can make use of neural networks to train on a subset of the possible states and generalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning can be combined with the use of a neural network as a Q-function approximator. This replaces the need for a lookup table and it can be applied to problems with a continuous state space. Deep Q-learning networks have produced extremely well-performing agents in games like chess, Go and even video games like Dota 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/q-learning-to-deep-q-network.png\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgammon Overview:\n",
    "One of reinforcement learning's most notable and earliest application was in backgammon in the 1990s. The TD-Gammon model became the best Backgammon player in the world in 1995.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"30%\">            \n",
    "            <img src=\"images/backgammon-board.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: left;\">\n",
    "                <strong>Rules Overview:</strong>\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">- The white player moves checkers in increasing numbers, the black player moves their checkers in decreasing numbers\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "- Two die are rolled on the start of a player's turn and the resulting number on each dice tells the player how many places they're allowed to move any of their checkers by\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "- Whoever moves their checkers off the board first wins the game\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "- You are blocked from moving a checker to any point where the opponent currently has 2 or more checkers stacked\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">\n",
    "- If your checker lands on a point where the opponent only has a single checker, then that checker is sent to the middle bar where the opponent must place it back on the starting position\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgammon Neural Network Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Representation:\n",
    "The goal is to represent the board state as an input vector to be fed into a feed-forward neural network. \n",
    "\n",
    "There are 24 possible possibles for checkers to occupy in the board. For both the white player and black player, we have:\n",
    "- One vector of size 24 where we store 1 if a position has exactly 1 checker and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has exactly 2 checkers and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has exactly 3 checkers and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has 4 or more checkers and 0 if not. \n",
    "- 1 input node for checkers that have been pushed to the middle bar by the opponent\n",
    "- 1 input node for checkers that have made it off the board\n",
    "\n",
    "This gives a total of $2(96+1+1)=196$ nodes in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why model it precisely this way and not some other way? &mdash; Having 1 piece in a position is considered dangerous because the opponent can potentially send it back to the start and delay your victory. Using 1-hot encoded vectors of size 24 separately based on how many checkers there are in a particular position lets the network treat positions with only 1 checker differently to how it treats positions with 0, 2, 3, 4, ... checkers.\n",
    "\n",
    "By applying our knowledge of the domain, we can represent the input in a way that lets the network generalise easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original network had 1 hidden layer consisting of 20 hidden nodes and 1 output layer consisting of 1 output node, using `sigmoid` to give a probability of winning the game at this current board state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Reinforcement Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector represents a *state*. The output vector represents the *value* of this state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the turn, the 2 die are rolled, then every possible board state following the resulting 2 die outcomes are determined, converted to an input vector, then fed through the neural network. Which next board state is chosen depends on which input led to the largest output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for the feed-forward network is: \n",
    "$$w := w + \\eta (T-V)\\frac{\\partial V}{\\partial w},$$ \n",
    "where $T$ is the target and $V$ is the predicted output.\n",
    "\n",
    "Since this is not supervised learning, we don't have access to a 'true' $T$ value. How do we know what the actual output $V$ *should've been*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the value of the next state $V_{t+1}$ as the expected value of $V_{t}$. This is what *temporal difference* learning does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD-Learning in Episodic Games:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backgammon is an *episodic* game, that is, the agent can only receive a reward at the end of the whole game. If the value at the closing state of the game is $V_{m+1}$, it typically has the value 1 for win and -1 for loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game can be reduced to a sequence of estimated values:\n",
    "\n",
    "$$\n",
    "    V_{t} \\to V_{t+1} \\to \\ldots \\to V_{m} \\to V_{m+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/reinforcement-learning-value-function-57b04e911152\">Value functions and rewards</a>\n",
    "- <a href=\"https://medium.com/free-code-camp/an-introduction-to-reinforcement-learning-4339519de419\">Reinforcement learning with Q-learning</a>\n",
    "- <a href=\"https://towardsdatascience.com/the-complete-reinforcement-learning-dictionary-e16230b7d24e\">Reinforcement learning terminology</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=nyjbcRQ-uQ8&list=PLZbbT5o_s2xoWNVdDudn51XM8lOuZ_Njv&ab_channel=deeplizard\">Video series on reinforcement learning</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
