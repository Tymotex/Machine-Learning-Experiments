{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *agent* interacts with its environment\n",
    "\n",
    "There is a set $S$ of *states* and a set $A$ of actions.\n",
    "\n",
    "At each time step $t$, the agent is in some state $s_t$ where it must choose an action $a_t$ to receive reward $r_t=R(s_t, a_t)$ transition to the next state $s_{t+1} = \\delta(s_t, a_t)$ \n",
    "- $\\delta$ is the state transition function\n",
    "- $R$ is the reward function\n",
    "- $\\delta$ and $R$ are properties of the *environment*\n",
    "\n",
    "The agent has a *policy* $\\pi: S \\to A$ that maps states into actions. The goal is to find the optimal policy $\\pi^*$ that maximises the cumulative reward.\n",
    "\n",
    "- REWORD? $\\delta$, $R$ and $\\pi$ can be *stochastic* rather than *determinstic*, that is, randomly decided in some way (eg. in a game with dice rolling mechanics). In this case, we write each function as probability distributions\n",
    "    - $\\delta(s_{t+1}=s|s_t, a_t)$, $R(r_t=r|s_t, a_t)$ and $\\pi(a_t=a|s_t)$\n",
    "    - In some  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models of Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's 3 standard ways for defining the optimality of short-term reward vs long-term reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Finite horizon reward* &mdash; when you set a 'stopping point' or 'horizon' sometime in the future and you only consider the sum of rewards *up to that point and not any further*\n",
    "    - Simpler computation, but could result in poorer choices\n",
    "$$\\sum_{i=1}^{h-1} \\big( r_t + i \\big)$$\n",
    "\n",
    "- *Infinite discounted reward* &mdash; summing rewards all the way in the future, but applying a *discount factor* $\\gamma$.\n",
    "    - Used most frequently\n",
    "$$\\sum_{i=1}^{\\infty} \\big( \\gamma^i r_{t+i} \\big), \\text{ where } 0 \\leq \\gamma < 1$$\n",
    "\n",
    "- *Average reward* &mdash; we pick some number $h$, add up all the rewards up to $h$, then take the average, and observe what the value is as $h$ approaches infinity\n",
    "$$\\lim_{h \\to \\infty} \\big( \\frac{1}{h} \\sum_{i=1}^{h-1} r_{t+i} \\big)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"50%\">    \n",
    "            <p>\n",
    "                Here is an <em>environment</em> which models a finite state machine. Following paths $a_1, a_2$ or $a_3$ leads to different reward loops. The best path is $a_3$\n",
    "            </p>\n",
    "            <img src=\"images/models-of-optimality-comparison.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p>\n",
    "                Finite horizon with $k=4$ &mdash; $a_1$ is preferred \n",
    "            </p>\n",
    "            <p>\n",
    "                Infinite discounted reward with $\\gamma=0.9$ &mdash; $a_2$ is preferred \n",
    "            </p>\n",
    "            <p>\n",
    "                Average reward with &mdash; $a_3$ is preferred \n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *reward* is immediate and it is what the agent wants to maximise. The *value* of a state $s$, $V(s)$, is a measurement of the potential future rewards from being in state $s$. Decisions made by the agent shouldn't always aim for highest short-term rewards but instead aim for entering the next state which has the highest *value*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/tic-tac-toe-value-states.png\" width=\"25%\">\n",
    "<em><p style=\"text-align: center;\">The agent picks the next state based on the highest value available</p></em>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose an agent is in state $s$ and chooses actions based on policy $\\pi$. Every policy $\\pi$ determines a *value function* that maps states to numbers, $V^{\\pi}:S\\to R$, with the value of a particular state is: \n",
    "$$V^{\\pi}(s) = \\text{average discounted reward received by the agent, if they begin in state } s.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an optimal policy function, $\\pi = \\pi^*$, the value $V^*(s)$ is the maximum discounted reward achievable at state $s$. The value $V(s)$ is an estimate of the 'true' value $V^*(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of *value function learning* is to start with a random $V$, then iteratively improve it, converging towards $V^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/env-policy-value-game.png\" width=\"100%\">\n",
    "<em>\n",
    "    <p style=\"text-align: center;\">\n",
    "        (a) is the environment, (b) is the policy, (c) is the value function defined by the policy\n",
    "    </p>\n",
    "</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration-Exploitation Tradeoff:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure convergence to the optimal strategy, the agent must try some *exploration* on unknown states, that is, risking a choice that could have poorer rewards to discover better rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploitation is sticking to choosing states that the agent knows will produce good rewards, however it may be sticking to a suboptimal choice because the optimal choice may still be undiscovered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon greedy strategy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two methods for deciding between exploration/exploitation:\n",
    "- Choosing a random action 5% of the time\n",
    "- Using softmax/Boltzmann distribution to systematically choose the next action:\n",
    "$$\n",
    "    P(a)=\\frac{e^{\\frac{R(a)}{T}}}{\\sum_{b \\in A} e^{\\frac{R(b)}{T}}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"As multiple actions can be taken at any given state, so constantly picking only one action at a state that used to bring success might end up missing other better states to be in. In reinforcement learning, this is the explore-exploit dilemma.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"With explore strategy, the agent takes random actions to try unexplored states which may find other ways to win the game. With exploit strategy, the agent is able to increase the confidence of those actions that worked in the past to gain rewards. With a good balance between exploring and exploiting, and by playing infinitely many games, the value for every state will approach its true probability. This good balance between exploring and exploit is determined by the epsilon greedy parameter.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example value function calculations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/delayed-reinforcement-example.png\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that the agent has no familiarity with the environment and no idea of what the rewards for each state transition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have an optimal policy that maps $\\pi(s_2)=a_1$ $\\pi(s_3)=a_2$. If $\\gamma = 0.9$, then the value of state $s_3$ is given by:\n",
    "\n",
    "$$V^\\pi(s_3)=2+0.9\\cdot 2 + 0.9^2 \\cdot 2 + ... = \\frac{2}{1-0.9}= 20.$$\n",
    "\n",
    "The value of state $s_2$ is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    V^\\pi(s_2) &=-1+0.9\\cdot 2 + 0.9^2 \\cdot 2 + \\ldots \\\\\n",
    "               &= -1 + 0.9(\\frac{2}{1-0.9}) = -1 + 0.9\\cdot V^\\pi (s_3) \\\\\n",
    "               &= 17.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The value of state $s_1$ is given by $V^\\pi(s_1) = -1 + 0.9 \\cdot V^\\pi (s_2) = 14.3$.\n",
    "\n",
    "What if the policy mapped $\\pi(s_1) = a_1$ instead? The value of state $s_1$ would be $V^\\pi(s_1) = 1 + 0.9 \\cdot 1 + 0.9^2 \\cdot 1 + ... = \\frac{1}{1-0.9}=10.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Algorithms:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 classical reinforcement learning algorithms: *temporal difference learning* and *Q-learning*.\n",
    "- *Temporal difference learning* tries to learn the optimal value function $V^*$\n",
    "- *Q-learning* tries to learn the optimal Q-function $Q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning (TD Learning):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $R$ and $\\delta$ are deterministic. The update rule is:\n",
    "\n",
    "$$\n",
    "    V(s_t) := r_t + \\gamma V(s_{t+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $R$ and $\\delta$ are stochastic instead. We have to apply this update rule instead, introducing the learning rate $\\eta$:\n",
    "\n",
    "$$\n",
    "    V(s_t) := V(s_t) + \\eta \\big( r_t + \\gamma V(s_{t+1}) - V(s_t) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: The fuck is q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a deterministic environment, the optimal policy $\\pi^*$, optimal Q-function $Q^*$ and optimal value function $V^*$ are related by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\pi^*(s) &= \\max_a \\big( Q^*(s, a) \\big) \\tag{1} \\\\\n",
    "    Q^*(s, a) &= \\underbrace{R(s, a)}_{\\text{immediate reward}} + \\gamma V^*\\big( \\underbrace{\\delta (s, a) \\big)}_{\\text{new state}} \\tag{2} \\\\ \n",
    "    V^*(s) &= \\max_b\\big( Q^*(s, b)\\big) \\tag{3}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining $(2)$ and $(3)$, we get the equation relating $Q^*$ back to itself:\n",
    "\n",
    "$$\n",
    "    Q^*(s, a) = R(s, a) + \\gamma \\max_b \\big( Q^*\\big(\\delta(s, a), b\\big) \\big),\n",
    "$$\n",
    "\n",
    "where $b$ is the action that maximises the Q-value of the next state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule in a deterministic environment follows from the above result:\n",
    "\n",
    "$$\n",
    "    Q(s_t, a_t) := r_t + \\gamma \\max_b \\big( Q(s_{t+1}, b) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the environment is stochastic, then we have the update rule:\n",
    "$$\n",
    "    Q(s_t, a_t) := Q(s_t, a_t) + \\eta \\big( r_t + \\gamma \\max_{b} \\big( Q(s_{t+1}, b) - Q(s_t, a_t) \\big) \\big)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem. Q-learning will eventually converge to the optimal policy for any deterministic Markov decision process, assuming an appropriately randomised strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem. TD-learning will also converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These theoretical results have impracticalities during training:\n",
    "- Delayed reinforcement &mdash; the discovery of better state transitions takes several timesteps\n",
    "- Search space must be finite &mdash; otherwise convergence is way too slow\n",
    "- Can't rely on lookup table for $Q$ values. Eg. this is impossible to scale up to something like chess where there is an unimaginably large set of possible states. Instead, we can make use of neural networks to train on a subset of the possible states and generalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q-function is similar to the value function, but it maps a state *and an action* to a reward: $Q^\\pi : S \\times A \\to R$, with\n",
    "\n",
    "$Q^\\pi(s, a)$ giving the average discounted reward received by the agent when they begin in state $s$ having forced to choose action $a$ at the __first timestep__ and then choosing actions based on policy $\\pi$ thereafter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/delayed-reinforcement-example.png\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s_2, a_2)$ is the value of state $s_2$, given that we were forced to choose $a_2$ at the first timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q^*(s_2, a_2) = 1 + 0.9\\cdot V^*(s_1) = 1+0.9\\cdot 14.3 = 13.87$.\n",
    "\n",
    "$Q^*(s_2, a_1) = V^*(s_2) = 17$, since if we were forced to pick $a_1$ at $s_2$, that would've been the same as what the optimal policy would have picked as well.\n",
    "\n",
    "$Q^*(s_3, a_1) = 1 + 0.9V^*(s_2) = 16.3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Game Playing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1:00:00ish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgammon Overview:\n",
    "\n",
    "<img src=\"images/backgammon-board.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- The white player moves checkers in increasing numbers, the black player moves their checkers in decreasing numbers\n",
    "\n",
    "- Two die are rolled on the start of a player's turn and the resulting number on each dice tells the player how many places they're allowed to move any of their checkers by\n",
    "\n",
    "- Whoever moves their checkers off the board first wins the game\n",
    "\n",
    "- You are blocked from moving a checker to any point where the opponent currently has 2 or more checkers stacked\n",
    "\n",
    "- If your checker lands on a point where the opponent only has a single checker, then that checker is sent to the middle bar where the opponent must place it back on the starting position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backgammon Neural Network Architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning's most notable early application was in backgammon in the 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input Representation:\n",
    "The goal is to represent the board state as an input vector to be fed into a feed-forward neural network. \n",
    "\n",
    "There are 24 possible possibles for checkers to occupy in the board. For both the white player and black player, we have:\n",
    "- One vector of size 24 where we store 1 if a position has exactly 1 checker and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has exactly 2 checkers and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has exactly 3 checkers and 0 if not.\n",
    "- One vector of size 24 where we store 1 if a position has 4 or more checkers and 0 if not. \n",
    "- 1 input node for checkers that have been pushed to the middle bar by the opponent\n",
    "- 1 input node for checkers that have made it off the board\n",
    "\n",
    "This gives a total of $2(96+1+1)=196$ nodes in the input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why model it precisely this way and not some other way? &mdash; Having 1 piece in a position is considered dangerous because the opponent can potentially send it back to the start and delay your victory. Using 1-hot encoded vectors of size 24 separately based on how many checkers there are in a particular position lets the network treat positions with only 1 checker differently to how it treats positions with 0, 2, 3, 4, ... checkers.\n",
    "\n",
    "By applying our knowledge of the domain, we can represent the input in a way that lets the network generalise easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original network had 1 hidden layer consisting of 20 hidden nodes and 1 output layer consisting of 1 output node, using `sigmoid` to give a probability of winning the game at this current board state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying Reinforcement Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input vector represents a *state*. The output vector represents the *value* of this state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the turn, the 2 die are rolled, then every possible board state following the resulting 2 die outcomes are determined, converted to an input vector, then fed through the neural network. Which next board state is chosen depends on which input led to the largest output value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule for the feed-forward network is: \n",
    "$$w := w + \\eta (T-V)\\frac{\\partial V}{\\partial w},$$ \n",
    "where $T$ is the target and $V$ is the predicted output.\n",
    "\n",
    "Since this is not supervised learning, we don't have access to a 'true' $T$ value. How do we know what the actual output $V$ *should've been*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the value of the next state $V_{t+1}$ as the expected value of $V_{t}$. This is what *temporal difference* learning does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD-Learning in Episodic Games:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backgammon is an *episodic* game, that is, the agent can only receive a reward at the end of the whole game. If the value at the closing state of the game is $V_{m+1}$, it typically has the value 1 for win and -1 for loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game can be reduced to a sequence of estimated values:\n",
    "\n",
    "$$\n",
    "    V_{t} \\to V_{t+1} \\to \\ldots \\to V_{m} \\to V_{m+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/reinforcement-learning-value-function-57b04e911152\">Value functions and rewards</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
