{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Analyses\n",
    "\n",
    "Regression is the modelling a *target parameter* based on *invidual predictors*.\n",
    "\n",
    "### Simple Linear Regression:\n",
    "Simple linear regression &mdash; where there is 1 independent predictor variable and is linearly related to the dependent target variable.\n",
    "\n",
    "<img src=\"images/linear-regression.png\" width=\"50%\">\n",
    "\n",
    "\n",
    "We can approximate the relationship between the predictor and the target with a line of best fit with equation: $y=a_0 + a_1x$.\n",
    "\n",
    "The linear regression algorithm's aim is to find the values $a_0$ and $a_1$ to get this line of best fit. It does this by minimising a loss/cost function such as the *mean squared error (MSE)* function: $L = \\frac{1}{n} \\sum_{i=1}^{n} (\\texttt{prediction}_i - y_i)^2$, where each $i$  in $1 \\leq i \\leq n$ represents a single datapoint and $\\texttt{prediction}_i$ is the corresponding prediction at the same x-value.\n",
    "\n",
    "We can use *gradient descent* to force $a_0$ and $a_1$ to converge on values that will minimise $L=\\frac{1}{n}\\sum_{i=1}^{n} \\big( a_0 + a_1 x_i - y_i \\big)^2$.\n",
    "\n",
    "Computing the partial derivatives with respect to $a_0$ and $a_1$:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_0}=\\frac{2}{n} \\sum_{i=1}^{n} \\big( a_0 + a_1 x_i - y_i \\big),$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial a_1}=\\frac{2}{n} \\sum_{i=1}^{n} \\big( a_0 + a_1 x_i - y_i \\big)\\cdot x_i.$$\n",
    "\n",
    "A derivative like $\\frac{\\partial L}{\\partial a_0}$ summarises how much $L$ changes when you 'nudge' the value of $a_0$ by an infinitesimally small amount.\n",
    "\n",
    "Now, to apply gradient descent, we repeatedly apply the following updates for each prediction we make:\n",
    "$$a_0 := a_0 - \\eta \\frac{\\partial L}{\\partial a_0},$$\n",
    "$$a_1 := a_1 - \\eta \\frac{\\partial L}{\\partial a_1}.$$\n",
    "\n",
    "Repeatedly updating the weights with gradient descent will force $a_0$ and $a_1$ to converge on values to minimise $L$.\n",
    "\n",
    "<hr />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression:\n",
    "<img src=\"images/logistic-vs-linear-regression.png\" width=\"50%\">\n",
    "\n",
    "The logistic regression algorithm involves passing the weighted sum (plus bias) through the logistic sigmoid activation function, mapping the weighted sum to a number between 0 and 1 &mdash; modelling a probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a computational graph of the l\n",
    "\n",
    "Forward propagation: calculating the weighted sum, plus bias, passing it through the logistic sigmoid function, then calculating the error with cross-entropy.\n",
    "\n",
    "Backward propagation: computing the derivative of the loss function $\\frac{\\partial L}{\\partial z}$ and the activation function $\\frac{\\partial z}{\\partial s}$, then using that to proportionally attribute error to each weight $w_1$, $w_2$ and bias $b$. \n",
    "\n",
    "<img src=\"images/logistic-regression-backprop-diagram.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://towardsdatascience.com/introduction-to-machine-learning-algorithms-linear-regression-14c4e325882a\">Linear regression</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
