{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Artist-Critic Co-Evolution:\n",
    "In the artist-critic co-evolution paradigm:\n",
    "- The *discriminator/critic* is rewarded for being able to distinguish real images from generated images\n",
    "- The *generator/artist* is rewarded for generating images that the critic cannot distinguish from being fake\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With generative adversarial networks, both the generator and discriminator are deep convolutional neural networks.\n",
    "- Generator: $G_\\theta : z \\to x$ &mdash; a network with parameters $\\theta$ that maps hidden layer vectors $z$, which were drawn from a standard normal distribution, to output images $x$\n",
    "- Discriminator: $D_\\psi: x \\to [0, 1]$ &mdash; a network with parameters $\\psi$ that maps generated images to either a value in the interval $[0, 1]$, indicating the probability of the image being real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Procedure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We alternate between feeding the discriminator real images and fake images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\min_\\theta \\Big( \\max_\\psi \\big( \\underbrace{E_{x\\text{~}p_{\\text{data}}}\\big(\\log(D_\\psi (x))\\big)}_{\\text{Input image's 'realness' rating}} + \\underbrace{E_{z\\text{~}p_{\\text{model}}}\\big( \\log(1-D_\\psi (G_\\theta(z)) ) \\big)  \\big)}_{\\text{Generated image's 'fakeness' rating}} \\Big) \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discriminator aims to maximise the inner expression &mdash; by having a high 'realness' prediction for input images and a high 'fakeness' rating for images produced by the generator, ie. it wants to have $D_\\psi (G_\\theta(z)) = 0$ so that the second term in $(1)$ is maximised.\n",
    "\n",
    "The generator aims to minimise what the discriminator maximises &mdash; it wants to generate images $G_\\theta(z)$ for which $D_\\psi(G_\\theta(z)) = 1$ so that the second term in the inner expression of $(1)$ is as negative as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the actual training of the network, we alternate between using gradient ascent on the discriminator:\n",
    "$$\n",
    "    \\max_\\psi\\Big(E_{x\\text{~}p_{\\text{data}}}\\big(\\log(D_\\psi(x)) \\big) + E_{z\\text{~}p_{\\text{model}}}\\big( \\log (1-D_\\psi(G_\\theta(z))) \\big) \\Big) \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and gradient ascent on the generator:\n",
    "$$\n",
    "    \\max_\\theta \\Big(E_{z\\text{~}p_{\\text{model}}}\\big( \\log (D_\\psi(G_\\theta(z))) \\big) \\Big). \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training loop:\n",
    "1. \n",
    "  1. Get minibatch of $m$ hidden layer vectors $z_1, z_2, \\ldots, z_m$ from $p(z)$\n",
    "  2. Get minibatch of $m$ training image vectors $x_1, x_2, \\ldots, x_m$ \n",
    "  3. Train the discriminator by gradient ascent on the network parameters $\\psi$ based on equation $(2)$:\n",
    "    $$\n",
    "        \\nabla_\\psi \\Bigg( \\frac{1}{m} \\sum_{i=1}^m \\Big( \\log(D_\\psi (x^{(i)})) + \\log (1 - D_\\psi(G_\\theta(z^{(i)}))) \\Big) \\Bigg)\n",
    "    $$\n",
    "2. \n",
    "  1. Get minibatch of $m$ hidden layer vectors $z_1, z_2, \\ldots, z_m$ from $p(z)$ based on equation $(1)$\n",
    "  2. Train the generator by gradient ascent on $\\theta$:\n",
    "    $$\n",
    "        \\nabla_\\theta \\Bigg( \\frac{1}{m} \\sum_{i=1}^m \\log \\big( D_\\psi (G_\\theta(z^{(i)})) \\big) \\Bigg)\n",
    "    $$\n",
    "    Note: this involves calculating and backpropagating the gradients from the discriminator into the generator network.\n",
    "    \n",
    "The discriminator and generator are trained separately each loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator Neural Network Architecture:\n",
    "<img src=\"images/gan-generator-architecture.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator's architecture is usually considered an 'inverted' convolution network. It starts with fully connected layers followed by convolutional layers.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
