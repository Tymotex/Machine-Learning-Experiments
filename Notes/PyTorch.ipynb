{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are multi-dimensional arrays with support for `autograd` operations like `backward()`. Tensors in PyTorch are similar to tensors in NumPy, except that they can be used on a GPU. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Tensors:\n",
    "\n",
    "- $\\texttt{torch.rand(shape)}$ &mdash; returns a tensor with random values drawn from a uniform distribution on interval $[0, 1)$\n",
    "- $\\texttt{torch.zeros(shape)}$ &mdash; returns a tensor where all components are initialised with zeroes\n",
    "- $\\texttt{torch.tensor(data)}$ &mdash; creates a tensor from the supplied data list/array \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Properties:\n",
    "\n",
    "Tensors have the following methods:\n",
    "- $\\texttt{size()}$ &mdash; returns a tuple-like object containing the tensor's shape\n",
    "- $\\texttt{transpose(dim0, dim1)}$ &mdash; returns a transposed tensor\n",
    "    - Eg. `x.transpose(0, 1)` transposes a 2D tensor\n",
    "- $\\texttt{reshape(shape)}$ &mdash; returns a reshaped tensor, containing the same data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Original tensor =====\n",
      "tensor([[0.1869, 0.9153, 0.8237],\n",
      "        [0.8429, 0.8415, 0.9888]])\n",
      "Size: torch.Size([2, 3])\n",
      "===== Transpose =====\n",
      "tensor([[0.1869, 0.8429],\n",
      "        [0.9153, 0.8415],\n",
      "        [0.8237, 0.9888]])\n",
      "tensor([[0.1869, 0.8429, 0.9153, 0.8415, 0.8237, 0.9888]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.rand((2, 3))\n",
    "print(\"===== Original tensor =====\")\n",
    "print(x)\n",
    "print(\"Size: {}\".format(x.size()))\n",
    "\n",
    "x = x.transpose(0, 1)\n",
    "print(\"===== Transpose =====\")\n",
    "print(x)\n",
    "\n",
    "print(\"===== Reshape ====\")\n",
    "x = x.reshape((1, 6))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor operations:\n",
    "\n",
    "The elementwise math operators `+`, `-`, `*`, `/` can be used on any two size-compatible tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([1., 2., 3.])\n",
      "y = tensor([4., 5., 6.])\n",
      "===== Operations =====\n",
      "x + y = tensor([5., 7., 9.])\n",
      "x - y = tensor([-3., -3., -3.])\n",
      "x * y = tensor([ 4., 10., 18.])\n",
      "x / y = tensor([0.2500, 0.4000, 0.5000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "y = torch.tensor([4.0, 5.0, 6.0])\n",
    "print(\"x = {}\".format(x))\n",
    "print(\"y = {}\".format(y))\n",
    "\n",
    "print(\"===== Operations =====\")\n",
    "print(\"x + y = {}\".format(x + y))\n",
    "print(\"x - y = {}\".format(x - y))\n",
    "print(\"x * y = {}\".format(x * y))\n",
    "print(\"x / y = {}\".format(x / y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting between torch tensor and numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== torch.Tensor to numpy.ndarray =====\n",
      "Torch tensor: tensor([1, 2, 3])\n",
      "Numpy array:  [1 2 3]\n",
      "===== numpy.ndarray to torch.Tensor =====\n",
      "Numpy array:  [1 2 3]\n",
      "Torch tensor: tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"===== torch.Tensor to numpy.ndarray =====\")\n",
    "x = torch.tensor([1, 2, 3])\n",
    "print(\"Torch tensor: {}\".format(x))\n",
    "x = x.numpy()\n",
    "print(\"Numpy array:  {}\".format(x))\n",
    "\n",
    "print(\"===== numpy.ndarray to torch.Tensor =====\")\n",
    "x = np.array([1, 2, 3])\n",
    "print(\"Numpy array:  {}\".format(x))\n",
    "x = torch.from_numpy(x)\n",
    "print(\"Torch tensor: {}\".format(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CUDA Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0929, 2.1365, 3.7604]], device='cuda:0')\n",
      "tensor([[1.0929, 2.1365, 3.7604]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.rand((1, 3), device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd &mdash; Automatic Differentiation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `autograd` package provides automatic differentiation for all operations on Tensors.\n",
    "\n",
    "- Enabling tracking:\n",
    "    - Setting `requires_grad=True` on a new tensor tracks all the computation done on it. Once the computations are done, you can call `.backward()` to have all the gradients computed automatically. \n",
    "- Disabling tracking:\n",
    "    - `.detach()` method prevents computation tracking\n",
    "    - Wrapping the code block in `with torch.no_grad()` blocks tracking for everything within the block. Useful for when we're testing the model rather than training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation tracking example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "y = tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "y.grad_fn = <AddBackward0 object at 0x7f785d98e1f0>\n",
      "z = tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "z.grad_fn = <MulBackward0 object at 0x7f785d9bc250>\n",
      "Theta of z = 27.0\n",
      "Theta.grad_fn = <MeanBackward0 object at 0x7f785d9bc3a0>\n",
      "dθ/dx = tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(\"x = {}\".format(x))\n",
    "\n",
    "y = x + 2\n",
    "print(\"y = {}\".format(y))\n",
    "print(\"y.grad_fn = {}\".format(y.grad_fn))   # y was created as a result of an operation on x, so it has a grad_fn\n",
    "\n",
    "z = y * y * 3\n",
    "print(\"z = {}\".format(z))\n",
    "print(\"z.grad_fn = {}\".format(z.grad_fn))\n",
    "\n",
    "theta = z.mean()\n",
    "print(\"Theta of z = {}\".format(theta))\n",
    "print(\"Theta.grad_fn = {}\".format(theta.grad_fn))\n",
    "\n",
    "# Doing backpropagation:  (note that calling backward() is only valid on a scalar)\n",
    "theta.backward()\n",
    "\n",
    "print(\"dθ/dx = {}\".format(x.grad))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{split}\n",
    "    \\frac{\\partial \\theta}{\\partial x} & = \\frac{\\partial \\theta}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} \\\\\n",
    "    & = \n",
    "        \\begin{pmatrix}\n",
    "            \\frac{1}{4} & \\frac{1}{4}\\\\\n",
    "            \\frac{1}{4} & \\frac{1}{4}\n",
    "        \\end{pmatrix}\n",
    "     \\cdot 6\\big(\n",
    "        \\begin{pmatrix}\n",
    "            x_{11} & x_{12} \\\\\n",
    "            x_{21} & x_{22}\n",
    "        \\end{pmatrix}\n",
    "     +\n",
    "        \\begin{pmatrix}\n",
    "            2 & 2 \\\\\n",
    "            2 & 2\n",
    "        \\end{pmatrix}\n",
    "    \\big) \\cdot \n",
    "        \\begin{pmatrix}\n",
    "            1 & 1 \\\\\n",
    "            1 & 1\n",
    "        \\end{pmatrix}\n",
    "    \\\\\n",
    "    & = \n",
    "        \\begin{pmatrix}\n",
    "            \\frac{1}{4} & \\frac{1}{4}\\\\\n",
    "            \\frac{1}{4} & \\frac{1}{4}\n",
    "        \\end{pmatrix}\n",
    "    \\cdot 6\\big(\n",
    "        \\begin{pmatrix}\n",
    "            1 & 1 \\\\\n",
    "            1 & 1\n",
    "        \\end{pmatrix}\n",
    "    +\n",
    "        \\begin{pmatrix}\n",
    "            2 & 2 \\\\\n",
    "            2 & 2\n",
    "        \\end{pmatrix}\n",
    "    \\big) \\cdot \n",
    "        \\begin{pmatrix}\n",
    "            1 & 1 \\\\\n",
    "            1 & 1\n",
    "        \\end{pmatrix} \\\\\n",
    "    & = \n",
    "        \\begin{pmatrix}\n",
    "            4.5 & 4.5 \\\\\n",
    "            4.5 & 4.5 \n",
    "        \\end{pmatrix}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Example: \n",
    "\n",
    "Below is a network that classifies handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pytorch-sample-network.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Network parameters\n",
      "    torch.Size([6, 1, 3, 3])\n",
      "    torch.Size([6])\n",
      "    torch.Size([16, 6, 3, 3])\n",
      "    torch.Size([16])\n",
      "    torch.Size([120, 576])\n",
      "    torch.Size([120])\n",
      "    torch.Size([84, 120])\n",
      "    torch.Size([84])\n",
      "    torch.Size([10, 84])\n",
      "    torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # 1 input image channel, 6 output channels, 3x3 convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # 6 input channels, 16 output channels, 3x3 kernels\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # Note: the backward() function is automatically defined by autograd\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window on conv1\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)      # Max pooling over a (2, 2) window on conv2\n",
    "        x = x.view(-1, self.num_flat_features(x))       # Flattening(?)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "# Network parameters are accessible under net.parameters()\n",
    "print(\"Network parameters\")\n",
    "params = list(net.parameters())\n",
    "for each_layer in net.parameters():\n",
    "    print(\"    {}\".format(each_layer.size()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer: tensor([[ 0.0034,  0.0779, -0.0674,  0.0440,  0.0717,  0.0629, -0.1683,  0.0298,\n",
      "         -0.1209,  0.0345]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Making a prediction, zeroing the gradients, then backpropagating:\n",
    "\n",
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(\"Output layer: {}\".format(out))\n",
    "\n",
    "net.zero_grad()   # Need to zero the gradients prior to backpropagation\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7599, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)     # a dummy target, for example\n",
    "target = target.view(1, -1)  # Flatten it to the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sequence of computations in a forward pass:\n",
    "\n",
    "<img src=\"images/pytorch-sample-feedforward-sequence.png\" width=\"50%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation:\n",
    "Now, calling `loss.backward()`, the whole computational graph is differentiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== conv1.bias.grad before backward() ===\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "=== conv1.bias.grad after backward() ===\n",
      "tensor([-0.0092,  0.0256,  0.0169, -0.0281, -0.0075,  0.0155])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('=== conv1.bias.grad before backward() ===')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('=== conv1.bias.grad after backward() ===')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using optimisers:\n",
    "All that's left to do at this point is to update the weights using an optimiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimiser = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# This goes in the training loop:\n",
    "optimiser.zero_grad()\n",
    "output = net(input)\n",
    "loss = criterion(ouptut, target)\n",
    "loss.backward()\n",
    "optimiser.step()      # step() proceeds with the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height: 2px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a href=\"https://pytorch.org/docs/stable/nn.html\">`Torch.nn`</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nn.Module` &mdash; base class for all neural network modules. Convenient for encapsulating parameters, keep track of state and has helpers for moving them to the GPU\n",
    "- `parameters()` &mdash; returns an iterator containing the network's parameters\n",
    "- `zero_grad()` &mdash; zeroes the gradient buffers of all parameters. It's necessary to zero the gradients prior to backpropagation because PyTorch accumulates gradients on subsequent backward passes by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions:\n",
    "There are several different error functions available in the `nn` package. They all take in an $\\texttt{(prediction, target)}$ pair and give back a value that indicates the magnitude of prediction error\n",
    "- `MSELoss` &mdash; mean squared error\n",
    "- `CrossEntropyLoss` &mdash; cross entropy error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers (from `torch.optim`):\n",
    "- `SGD(net.parameters(), lr, momentum)`\n",
    "- `Adam([var, var2], lr)`\n",
    "\n",
    "#### Optimiser methods:\n",
    "- `zero_grad()`\n",
    "- `step()` &mdash; updates the network parameters. Called once the gradients have been computed by `backward()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Blocks:\n",
    "\n",
    "- `Linear(in_size, out_size)` &mdash; applies linear transformation: $y=xA^T+b$\n",
    "- `Conv2d(in_channels, out_channels, kernel_size, stride, padding)` &mdash; a 2D convolutional layer\n",
    "- `MaxPool2d()`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
