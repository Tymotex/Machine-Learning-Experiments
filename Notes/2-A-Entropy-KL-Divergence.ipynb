{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and Kullback-Leibler Divergence:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy:\n",
    "In statistics, thermodynamics and information theory, *entropy* is a measure of uncertainty or information (the connection between uncertainty and information is that \"an occurrence of an unlikely event gives you more *information* than the occurrence of a likely event\"). The *entropy* of a discrete probability distribution with $n$ different outcomes, $p=\\langle p_1, ..., p_n \\rangle$, is:\n",
    "\n",
    "$$H(p)=-\\sum_{i=1}^{n} p_i \\cdot \\log_{2}p_i. \\tag{1}$$\n",
    "\n",
    "For a continuous probability distribution, $p()$, the entropy is given by:\n",
    "\n",
    "$$H(P)=-\\int_\\theta p(\\theta) \\cdot \\log p(\\theta) d\\theta.$$\n",
    "\n",
    "For a *multivariable* Normal distribution:\n",
    "\n",
    "$$H(p)=\\sum_i \\log \\sigma_i.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-Entropy:\n",
    "Cross-entropy is a measure of the relative entropy between two probability distributions over the same set of events.\n",
    "\n",
    "$$\n",
    "    H(p, q)=-\\sum_{i=1}^{n} p_i \\cdot \\log_{2}q_i. \\tag{2}\n",
    "$$\n",
    "\n",
    "The cross-entropy is simply equal to the entropy, if the distributions $p$ and $q$ are equal. If the distributions $p$ and $q$ differ, then the cross-entropy will be larger than the entropy by some amount. The amount by which the cross-entropy value exceeds the entropy value is called the *relative entropy*, or *KL-divergence*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### KL-Divergence:\n",
    "\n",
    "For two probability distributions: $p=\\langle p_1, ..., p_n \\rangle$ and $q=\\langle q_1, ..., q_n \\rangle$, the *Kullback-Leibler Divergence* between $p$ and $q$ is:\n",
    "\n",
    "$$\n",
    "    \\begin{align}\n",
    "    D_{KL}(p \\parallel q) &= \\underbrace{H(p, q)}_{\\text{Cross-entropy}} - \\underbrace{H(p)}_{\\text{Entropy}}, \\\\\n",
    "    &= \\sum_{i=1}^{n} p_i (\\log_2 p_i - \\log_2 q_i).\\tag{3}\n",
    "    \\end{align}\n",
    "$$\n",
    "\n",
    "This gives a measure of similarity or 'distance' between two probability distributions. Note that KL-divergence is not symmetric: $D_{KL}(p \\parallel q) \\neq D_{KL}(q \\parallel p)$. \n",
    "\n",
    "<img src=\"images/kl-divergence-illustrated.png\" width=\"50%\">\n",
    "<p style=\"text-align: center;\"><em>The KL-divergence value, $D_{KL}(P \\parallel Q)$ is the measure of the 'mismatch' when we use probability distribution $Q$ to approximate the true probability distribution $P$</em></p>\n",
    "\n",
    "\n",
    "For continuous distributions, $p$ and $q$, KL-divergence is given by:\n",
    "\n",
    "$$D_{KL}(p \\parallel q) = \\int_\\theta p(\\theta)(\\log p(\\theta) - \\log q(\\theta)) d\\theta.$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Calculations:\n",
    "\n",
    "Suppose we have a true probability distribution of different weather conditions and a predicted probability distribution: \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>   \n",
    "            <p style=\"text-align: left;\">True probability distribution $p$</p>\n",
    "            <img src=\"images/entropy-weather-projection.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <p style=\"text-align: left;\">Predicted probability distribution $q$</p>\n",
    "            <img src=\"images/entropy-weather-projection-predicted.png\" width=\"100%\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entropy is of the true probability distribution is:\n",
    "$$\n",
    "    \\begin{align}\n",
    "    H(p) = &-\\sum_{i=1}^{n} p_i \\log_{2}p_i, \\\\\n",
    "     = &-\\big( 0.35 \\log_2 (0.35) + 0.35 \\log_2 (0.35)\\\\\n",
    "     &+ 0.1 \\log_2 (0.1) + 0.1 \\log_2 (0.1)\\\\\n",
    "     &+ 0.04 \\log_2 (0.04) + 0.04 \\log_2 (0.04)\\\\\n",
    "     &+ 0.01 \\log_2 (0.01) + 0.01 \\log_2 (0.01)\\big) = 2.22897\\ldots.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cross-entropy is given by:\n",
    "$$\n",
    "    \\begin{align}\n",
    "    H(p, q) = &-\\sum_i p_i \\log_2 q_i \\\\\n",
    "            = &-\\big( 0.35\\log_2 0.01 + 0.35\\log_2 (0.01)\\\\\n",
    "            &+ 0.1\\log_2 0.04 + 0.1\\log_2 0.04 \\\\\n",
    "            &+ 0.04\\log_2 0.1 + 0.04\\log_2 0.1 \\\\\n",
    "            &+ 0.01\\log_2 0.35 + 0.01\\log_2 0.35 \\big) = 5.87551\\ldots.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-divergence is given by:\n",
    "$$\n",
    "    \\begin{align}\n",
    "    D_{KL}(p \\parallel q) &= \\underbrace{H(p, q)}_{\\text{Cross-entropy}} - \\underbrace{H(p)}_{\\text{Entropy}} \\\\\n",
    "        &= 5.87551\\ldots - 2.22897\\ldots \\\\\n",
    "        &= 3.64654\\ldots.\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another Example Calculation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of deep learning, suppose we an image classifer with 5 output classes and we are using cross-entropy loss as the error function.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>            \n",
    "            <img src=\"images/entropy-animals-categoriser-table.png\" width=\"100%\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/entropy-animals-categoriser-dog.png\" width=\"20%\">\n",
    "            <p style=\"text-align: left;\">            \n",
    "                Suppose the true label vector is $p=[1, 0, 0, 0, 0]$ and the network's output vector is $q=[0.4, 0.3, 0.05, 0.05, 0.2]$.\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">            \n",
    "                The cross-entropy loss value is calculated as:\n",
    "                $$\n",
    "                    \\begin{align}\n",
    "                    H(p, q) &= -\\big( 1\\cdot\\ln (0.4) + 0 + 0 + 0 + 0 \\big)  \\\\\n",
    "                            &= 0.91629\\ldots.\n",
    "                    \\end{align}\n",
    "                $$\n",
    "            </p>\n",
    "            <p style=\"text-align: left;\">            \n",
    "                Suppose now after training, the network's predictions are more accurate and produces the output vector $q=[0.98, 0.01, 0, 0, 0.01]$. Here the cross-entropy is:\n",
    "                $$\n",
    "                    \\begin{align}\n",
    "                    H(p, q) &= -\\big( 1\\cdot\\ln (0.98) + 0 + 0 + 0 + 0 \\big)  \\\\\n",
    "                            &= 0.020207\\ldots.\n",
    "                    \\end{align}\n",
    "                $$\n",
    "            </p>\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Huffman Encoding:\n",
    "\n",
    "The concept of *entropy* was first introduced by Claude Shannon in 1948 in his paper title \"*A Mathematical Theory of Communication*\". The aim was to investigate the __smallest possible average size of lossless encoding__ for messages transmitted from a source to a destination.\n",
    "\n",
    "<img src=\"images/src-dest-message-entropy.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "The entropy value of a frequency distribution of characters to send in a message, $H(p)$, gives the *average number of bits* required to represent a character drawn randomly from the message, in the most efficient character encoding scheme generated by Huffman's encoding algorithm.\n",
    "\n",
    "__*Example 1.*__ Suppose a message consists only of $A$ and $B$, both with equal frequency. The huffman encoding would assign $A=\\texttt{0}$ and $B=\\texttt{1}$, for example. \n",
    "\n",
    "- The entropy value here would be $H(\\langle 0.5, 0.5 \\rangle)=-(0.5 \\cdot \\log_2\\frac{1}{2} + 0.5 \\cdot \\log_2\\frac{1}{2}) = 1 \\texttt{ bit}$ .\n",
    "\n",
    "__*Example 2.*__ Suppose a message consists of $A, B \\text{ and } C$ with frequencies $0.5, 0.25 \\text{ and } 0.25$ respectively. The huffman encoding would assign $A=0, B=10, C=11$, for example.\n",
    "\n",
    "- The entropy value here would be $H(\\langle 0.5, 0.25, 0.25 \\rangle)=-(0.5 \\cdot \\log_2\\frac{1}{2} + 0.25 \\cdot \\log_2\\frac{1}{4} + 0.25 \\cdot \\log_2\\frac{1}{4}) = 1.5 \\texttt{ bits}$. \n",
    "\n",
    "\n",
    "Note: $D_{KL}(q \\parallel p)$ is the number of 'extraneous' bits that would be transmitted if we designed an encoding scheme based on $q's$ frequency distribution but it turned out the samples would be drawn from $p's$ frequency distribution instead.\n",
    "\n",
    "- Eg. If we designed a huffman encoding for the alphabet around the frequency distribution in English, but applied for German text, then the number of 'wastage' bits would be $D_{KL}(q \\parallel p)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward/Reverse KL-Divergence:\n",
    "*Forward KL-Divergence* &mdash; when we have a distribution $P$ and we want to choose a Normal distribution $Q$ which is 'close' to $P$, or 'approximately' $P$. In this case, we can use the KL-divergence, $D_{KL}(P \\parallel Q)$, as a loss function to minimise.\n",
    "\n",
    "*Reverse KL-Divergence* &mdash; when we have a distribution $P$ and we want to choose a Normal distribution $Q$ that minimises $D_{KL}(Q \\parallel P)$ rather than $D_{KL}(P \\parallel Q)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward             |  Reverse\n",
    ":-------------------------:|:-------------------------:\n",
    "![](images/forward-kl-divergence.png)   |  ![](images/reverse-kl-divergence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy in Deep Learning:\n",
    "Cross-entropy is a measure of the relative entropy between two probability distributions over the same set of events.\n",
    "\n",
    "For classification tasks where the output should be either 0 or 1, the mean squared error loss function works poorly. \n",
    "\n",
    "Instead, we can use the cross-entropy error function $E=-(t\\log (z) + (1-t)\\log(1-z))$, where $z$ is the network's probability prediction and $t$ is the target value.\n",
    "\n",
    "- If $t=1$, $E=-\\log (z)$\n",
    "- If $t=0$, $E=-\\log(1-z)$, so if $z\\approx 1$, then the loss value is very large\n",
    "\n",
    "This forces the network to put higher emphasis on misclassifications \n",
    "\n",
    "- Eg. In the case of detecting credit card fraud, there would be a significantly larger proportion of negative instances than positive instances. Cross-entropy would place greater emphasis on the positive instances it misclassifies, which in this application, is extremely important.\n",
    "\n",
    "Choosing the cross-entropy error function makes backpropagation computations simpler. Suppose we have the logistic sigmoid activation function: $a=\\frac{1}{1+e^{-z}}$. Note how\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial a}=\\frac{a-t}{a(1-a)},$$\n",
    "and applying the chain rule, we have\n",
    "$$\\frac{\\partial E}{\\partial z}=\\frac{\\partial E}{\\partial a}\\cdot \\frac{\\partial a}{\\partial z}=a-t,$$\n",
    "\n",
    "a very simple result.\n",
    "\n",
    "Note: we use $\\log_e$ in the cross-entropy code implementation rather than $\\log_2$ since the derivative of $\\ln$ is simpler than $\\log_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://www.youtube.com/watch?v=ErfnhcEV1O8&ab_channel=Aur%C3%A9lienG%C3%A9ron\">Introduction to entropy, cross-entropy and KL-divergence</a>\n",
    "- <a href=\"https://towardsdatascience.com/cross-entropy-for-dummies-5189303c7735\">Cross-entropy for dummies</a>\n",
    "- <a href=\"https://naokishibuya.medium.com/demystifying-entropy-f2c3221e2550\">Demystifying Entropy</a>\n",
    "- <a href=\"https://naokishibuya.medium.com/demystifying-kl-divergence-7ebe4317ee68\">Demystifying KL-Divergence</a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
