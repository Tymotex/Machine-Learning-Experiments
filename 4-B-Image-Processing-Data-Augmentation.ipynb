{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random image cropping:\n",
    "    - Taking multiple croppings of the same image (without losing the meaningful data in them), passing each through the network, then averaging the resultant loss can improve the knowledge gained from an image\n",
    "    - In the case of AlexNet, convolutional filters with a stride of 4 are used in their first convolutional layer. Even if some input features are skipped for one input, applying the convolution filtering on multiple random croppings should factor in those input features that were skipped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with deeper networks:\n",
    "\n",
    "Deep neural networks with tens of layers can be very sensitive to initial random weights.\n",
    "\n",
    "- 10+ layers:\n",
    "    - Weight initialisation &mdash; choosing intial weights that will make backpropagation work better. Normally, we initialise weights by drawing values at random from a normal distribution with $\\mu = 0$. With weight initialisation, we're selecting the value for standard deviation $\\sigma$ more carefully. \n",
    "    - Batch normalisation &mdash;\n",
    "- 30+ layers:\n",
    "    - Skip connections &mdash;\n",
    "- 100+ layers:\n",
    "    - Identity skip connections &mdash;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Suppose $y = \\sum_{k=1}^{n} w_k x_k$. If $x_k, x_k$ are independent, then \n",
    "$$\n",
    "    \\text{Var}(y) = n\\text{Var}(w)\\text{Var}(x). \\tag{1}\n",
    "$$\n",
    "\n",
    "Consider one layer $(i)$ of a deep network, with values:\n",
    "- weights $w_{jk}^{(i)}$\n",
    "- activations $x_{k}^{(i)}$ coming in from the previous layer from $1 \\leq k \\leq n_{i}$ with $n_i$ being the number of nodes in layer $(i)$\n",
    "- activations $x_{j}^{(i+1)}$ at the next layer from $1 \\leq j \\leq n_{i+1}$ with $n_{i+1}$ being the number of nodes in layer $(i+1)$\n",
    "\n",
    "The activation value at each node in the next layer is given by:\n",
    "\n",
    "$$\n",
    "    x_j^{(i+1)} = \\texttt{activation}(\\sum_{k=1}^{n_i} w_{jk}^{(i)} x_{k}^{(i)}). \\tag{2}\n",
    "$$\n",
    "\n",
    "From $(1)$ and $(2)$, we have \n",
    "\n",
    "$$\n",
    "    \\text{Var}(\\sum_{k=1}^{n_i} w_{jk}^{(i)} x_{k}^{(i)}) = n_i \\text{Var}(w_{jk}^{(i)}) \\text{Var}(x_{k}^{(i)}),\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\text{Var}(x^{(i+1)}) \\approx G_0 n_i \\text{Var}(w^{(i)}) \\text{Var}(x^{(i)}), \\tag{3}\n",
    "$$\n",
    "where $G_0$ is a constant to account for the activation function &mdash; normally we just set it to $G_0 = \\frac{1}{2}$.\n",
    "\n",
    "Suppose a network has $D$ layers, with input vector $x^{(i)}$ and output $z$. From $(3)$ collected across $D$ layers, we have\n",
    "\n",
    "$$\n",
    "    \\text{Var}(z) \\approx \\underbrace{\\prod_{i=1}^{D} \\big( G_0 n_i \\text{Var}(w_i) \\big)}_{\\text{We want this to be } \\approx \\space 1} \\cdot \\text{Var}(x). \\tag{4}\n",
    "$$\n",
    "\n",
    "If the product in $(4)$ is less than 1, then the activations going from input towards output will exponentially decay. If the product in $(4)$ is greater than 1, then the activations will exponentially grow going from input layer to output layer.\n",
    "\n",
    "Similarly, for backpropagation we have\n",
    "\n",
    "$$\n",
    "    \\text{Var}(\\frac{\\partial}{\\partial x}) \\approx \\underbrace{\\prod_{i=1}^{D} \\big( G_1 n_i \\text{Var}(w_i) \\big)}_{\\text{We want this to be } \\approx \\space 1} \\cdot \\text{Var}(\\frac{\\partial}{\\partial z}), \\tag{5}\n",
    "$$\n",
    "\n",
    "where we want the differentials to also not exponentially grow/decay too much across layers.\n",
    "\n",
    "The basis for deciding weights in *weight initialisation* is choosing weights $w_{jk}^{(i)}$ for all layers $i$ such that: \n",
    "\n",
    "$$\n",
    "    G_1 n_i \\text{Var}(w^{(i)}) = 1, \\tag{6}\n",
    "$$\n",
    "where $G_1 = \\frac{1}{2}$, usually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "<img src=\"images/weight-initialisation-1.png\" width=\"50%\">\n",
    "<em><p style=\"text-align: center;\">Error on the y axis</p></em>\n",
    "\n",
    "The above graph shows the difference in training speed for a 22-layer $\\texttt{ReLU}$ network for when suboptimal initial weights are chosen (blue line) vs. when weights are chosen according to $(6)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalisation:\n",
    "\n",
    "Technique for making deeper networks faster and stabler by recentering and rescaling the inputs to chosen layers in the network (normalisation) for each *mini-batch* .\n",
    "\n",
    "- Standardisation &mdash; making the mini-batch have $\\mu = 0$ and $\\sigma = 1$ through $z=\\frac{x-\\mu_d}{\\sigma_d}$ where $\\mu_d$ and $\\sigma_d$ are calculated from the current mini-batch\n",
    "    - Without standardising input values, having disproportionately large input values will cause the exploding gradient problem. A large activation value in an earlier layer will be transferred onto the next layer, cascading through the network and causing unstable weight changes\n",
    "- Covariate Shift &mdash; Covariate shift is the change in the distribution of input values. This may happen, for instance, if the training set comes from images downloaded from the web while the test set comes from an iPhone camera.\n",
    "    - The aim of batch normalisation is to reduce covariate shift by normalising the activations of each layer.\n",
    "    - Batch normalisation reduces the impact that earlier layers have on the distribution of inputs into later hidden layers. Ie. this 'decouples' the later layer's parameters from the earlier's layer's parameters\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "Normalising the activations of each layer to have mean $\\mu = 0$ and standard deviation $\\sigma = 1$ may limit the expressive power of the network.\n",
    "\n",
    "Because of this, we introduce addtional trainable parameters $\\gamma$ and $\\beta$.\n",
    "\n",
    "\n",
    "\n",
    "#### Algorithm:\n",
    "\n",
    "Suppose we have a batch of size $m$\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td width=\"60%\">            \n",
    "            <img src=\"images/batch-normalisation.png\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"images/batch-norm-gamma-beta.png\">\n",
    "        </td>\n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Where $\\mu_{B}$ is the mean of the output predictions, $\\sigma_{B}^2$ is the variance of the output predictions.\n",
    "\n",
    "Each output prediction $\\hat{x}_i$ is then normalised using the formula $\\hat{x}_i = \\frac{x_i-\\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$ where $\\epsilon$ is just a parameter to prevent zero division error. After this, all outgoing activation values have been standardised on this layer &mdash; $N(0, 1)$.\n",
    "\n",
    "The final output takes this activation and 'calibrates' it with new learnable parameters $\\gamma$ and $\\beta$: $y_i = \\gamma \\hat{x}_i + \\beta$.\n",
    "\n",
    "\n",
    "- The $\\gamma$ and $\\beta$ for each neuron is often initialised to be $1$ and $0$ respectively\n",
    "\n",
    "- This introduces more independent parameters to the network, but it keeps the activations within a 'healthy' range\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Connections:\n",
    "\n",
    "Skip connections are connections that bypass intermediate layers, so the output of one layer is fed directly as the input into some later layer.\n",
    "\n",
    "<img src=\"images/skip-connection.png\" width=\"25%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Style Transfer:\n",
    "\n",
    "\n",
    "... TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "- <a href=\"https://mlexplained.com/2018/01/10/an-intuitive-explanation-of-why-batch-normalization-really-works-normalization-in-deep-learning-part-1/\">Intuitive explanation of why batch normalisation works</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
