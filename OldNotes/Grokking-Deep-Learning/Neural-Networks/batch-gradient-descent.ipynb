{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent:\n",
    "A method for increasing the speed of training and rate of convergence.\n",
    "\n",
    "The following neural network implements both batch gradient descent and dropout regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration: 0 | Train Error: 1.0583443728359492 | Train Acc: 0.126 | Test Error: 0.936 | Test Acc: 0.3081\n",
      " Iteration: 10 | Train Error: 0.6012323748188507 | Train Acc: 0.672 | Test Error: 0.664 | Test Acc: 0.7214\n",
      " Iteration: 20 | Train Error: 0.5026413085978018 | Train Acc: 0.773 | Test Error: 0.640 | Test Acc: 0.7723\n",
      " Iteration: 30 | Train Error: 0.44342354131984413 | Train Acc: 0.814 | Test Error: 0.629 | Test Acc: 0.8\n",
      " Iteration: 40 | Train Error: 0.40893057260581783 | Train Acc: 0.827 | Test Error: 0.600 | Test Acc: 0.8194\n",
      " Iteration: 50 | Train Error: 0.37951504874263114 | Train Acc: 0.855 | Test Error: 0.620 | Test Acc: 0.8321\n",
      " Iteration: 60 | Train Error: 0.3536579963111288 | Train Acc: 0.863 | Test Error: 0.595 | Test Acc: 0.8435\n",
      " Iteration: 70 | Train Error: 0.33587070217264886 | Train Acc: 0.887 | Test Error: 0.602 | Test Acc: 0.8509\n",
      " Iteration: 80 | Train Error: 0.3186871719757355 | Train Acc: 0.884 | Test Error: 0.619 | Test Acc: 0.858\n",
      " Iteration: 90 | Train Error: 0.30607996529548365 | Train Acc: 0.897 | Test Error: 0.607 | Test Acc: 0.8623\n",
      " Iteration: 100 | Train Error: 0.28657349959496536 | Train Acc: 0.919 | Test Error: 0.617 | Test Acc: 0.8668\n",
      " Iteration: 110 | Train Error: 0.2832406630129957 | Train Acc: 0.911 | Test Error: 0.599 | Test Acc: 0.8702\n",
      " Iteration: 120 | Train Error: 0.2720817729122045 | Train Acc: 0.915 | Test Error: 0.638 | Test Acc: 0.8706\n",
      " Iteration: 130 | Train Error: 0.26186096882782417 | Train Acc: 0.93 | Test Error: 0.629 | Test Acc: 0.8721\n",
      " Iteration: 140 | Train Error: 0.2703037064097863 | Train Acc: 0.927 | Test Error: 0.622 | Test Acc: 0.8752\n",
      " Iteration: 150 | Train Error: 0.2508433803424021 | Train Acc: 0.933 | Test Error: 0.636 | Test Acc: 0.8753\n",
      " Iteration: 160 | Train Error: 0.26423419364127243 | Train Acc: 0.93 | Test Error: 0.614 | Test Acc: 0.8781\n",
      " Iteration: 170 | Train Error: 0.2467134088376004 | Train Acc: 0.941 | Test Error: 0.630 | Test Acc: 0.8798\n",
      " Iteration: 180 | Train Error: 0.2452061176089521 | Train Acc: 0.946 | Test Error: 0.631 | Test Acc: 0.8812\n",
      " Iteration: 190 | Train Error: 0.2323532713151192 | Train Acc: 0.952 | Test Error: 0.656 | Test Acc: 0.8807\n",
      " Iteration: 200 | Train Error: 0.2317545479170767 | Train Acc: 0.949 | Test Error: 0.627 | Test Acc: 0.8818\n",
      " Iteration: 210 | Train Error: 0.23568556650000902 | Train Acc: 0.947 | Test Error: 0.652 | Test Acc: 0.8829\n",
      " Iteration: 220 | Train Error: 0.22285289501119793 | Train Acc: 0.95 | Test Error: 0.663 | Test Acc: 0.8819\n",
      " Iteration: 230 | Train Error: 0.21336861059808812 | Train Acc: 0.964 | Test Error: 0.649 | Test Acc: 0.8834\n",
      " Iteration: 240 | Train Error: 0.2154320136173697 | Train Acc: 0.961 | Test Error: 0.655 | Test Acc: 0.8831\n",
      " Iteration: 250 | Train Error: 0.20978311921399395 | Train Acc: 0.963 | Test Error: 0.660 | Test Acc: 0.8817\n",
      " Iteration: 260 | Train Error: 0.2117300549074801 | Train Acc: 0.961 | Test Error: 0.679 | Test Acc: 0.8839\n",
      " Iteration: 270 | Train Error: 0.20303442140986375 | Train Acc: 0.973 | Test Error: 0.656 | Test Acc: 0.8836\n",
      " Iteration: 280 | Train Error: 0.208979937697514 | Train Acc: 0.965 | Test Error: 0.668 | Test Acc: 0.8844\n",
      " Iteration: 290 | Train Error: 0.20374696117163535 | Train Acc: 0.964 | Test Error: 0.689 | Test Acc: 0.8838\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# === Preparing the datasets ===\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "# Flatten the training images to an array of 784 pixels. Take 1000 sample datapoints\n",
    "images, labels = (x_train[0 : 1000].reshape(1000, 28 * 28) / 255, y_train[0 : 1000])\n",
    "one_hot_labels = np.zeros((len(labels), 10))\n",
    "for i, label in enumerate(labels):\n",
    "    one_hot_labels[i][label] = 1\n",
    "labels = one_hot_labels\n",
    "test_images = x_test.reshape(len(x_test), 28 * 28) / 255\n",
    "test_labels = np.zeros((len(y_test), 10))\n",
    "for i, label in enumerate(y_test):\n",
    "    test_labels[i][label] = 1\n",
    "\n",
    "# Relu functions\n",
    "def relu(x):\n",
    "    return (x > 0) * x\n",
    "\n",
    "def reluDerivative(x):\n",
    "    return (x > 0)\n",
    "\n",
    "alpha, iterations = (0.001, 300)\n",
    "pixels_per_image, num_labels, hidden_size = (784, 10, 100)\n",
    "\n",
    "# Initialise weights (scaled from random weights in [0, 1] to random weights in [-0.1, 0.1])\n",
    "np.random.seed(1)\n",
    "weights_0_1 = 0.2 * np.random.random((pixels_per_image, hidden_size)) - 0.1\n",
    "weights_1_2 = 0.2 * np.random.random((hidden_size, num_labels)) - 0.1\n",
    "\n",
    "# Batch size of 100\n",
    "batch_size = 100\n",
    "for i in range(iterations):\n",
    "    train_error, correct_count = (0.0, 0)\n",
    "    for j in range(int(len(images) / batch_size)):\n",
    "        # Getting the left and right slice bounds for the current batch\n",
    "        batch_start = j * batch_size\n",
    "        batch_end = (j + 1) * batch_size\n",
    "        \n",
    "        layer_0 = images[batch_start : batch_end]\n",
    "        layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "        layer_1_dropout_mask = np.random.randint(2, size=layer_1.shape)\n",
    "        layer_1 *= layer_1_dropout_mask\n",
    "        layer_2 = np.dot(layer_1, weights_1_2)\n",
    "        \n",
    "        train_error += np.sum((layer_2 - labels[batch_start : batch_end]) ** 2)\n",
    "        for k in range(batch_size):\n",
    "            correct_count += int(np.argmax(layer_2[k : k + 1]) == np.argmax(labels[batch_start + k : batch_start + k + 1]))\n",
    "            layer_2_delta = (labels[batch_start : batch_end] - layer_2) / batch_size\n",
    "            layer_1_delta = np.dot(layer_2_delta, weights_1_2.T) * reluDerivative(layer_1)\n",
    "            layer_1_delta *= layer_1_dropout_mask\n",
    "            weights_1_2 += alpha * np.dot(layer_1.T, layer_2_delta)\n",
    "            weights_0_1 += alpha * np.dot(layer_0.T, layer_1_delta)\n",
    "    if i % 10 == 0:\n",
    "        test_error, test_correct_count = (0.0, 0)\n",
    "        for m in range(len(test_images)):\n",
    "            layer_0 = test_images[m : m + 1]\n",
    "            layer_1 = relu(np.dot(layer_0, weights_0_1))\n",
    "            layer_2 = np.dot(layer_1, weights_1_2)\n",
    "            test_correct_count += int(np.argmax(layer_2) == np.argmax(test_labels[m : m + 1]))\n",
    "            test_error += np.sum((layer_2 - test_labels[m : m + 1]) ** 2)\n",
    "        print(\"\\r\"+ \\\n",
    "              \" Iteration: {}\".format(i) + \\\n",
    "              \" | Train Error: \" + str(train_error / float(len(images))) + \\\n",
    "              \" | Train Acc: \" + str(correct_count / float(len(images))) + \\\n",
    "              \" | Test Error: \" + str(test_error / float(len(test_images)))[0:5] + \\\n",
    "              \" | Test Acc: \" + str(test_correct_count / float(len(test_images))))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a batch_size of 100, the CPU is doing 100 vector dot products at once for each batch of 100 datapoints. This is faster than individually computing dot products. Alpha is also allowed to be larger because taking the average weight change over 100 samples generally settles on a consistent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
